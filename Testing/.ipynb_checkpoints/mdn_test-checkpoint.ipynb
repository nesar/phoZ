{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/nramachandra/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "from astropy.table import Table\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib.axes as axes;\n",
    "from matplotlib.patches import Ellipse\n",
    "import seaborn as sns;\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "import SetPub\n",
    "SetPub.set_pub()\n",
    "\n",
    "\n",
    "# https://github.com/dunovank/jupyter-themes\n",
    "# jt -t onedork -fs 14 -altp -tfs 14 -nfs 14 -ofs 14 -cellw 90% -T -N -kl\n",
    "\n",
    "# # import jtplot submodule from jupyterthemes\n",
    "# from jupyterthemes import jtplot\n",
    "\n",
    "# # currently installed theme will be used to\n",
    "# # set plot style if no arguments provided\n",
    "# jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n",
      "=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n"
     ]
    }
   ],
   "source": [
    "print(20*'=~')\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "print(20*'=~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadGalaxPy(path_program = '../../Data/fromGalaxev/photozs/datasets/', sim_obs_combine = True):    \n",
    "    class Curated_sample():\n",
    "        ''' Class to store the redshift and colors of observed galaxies,\n",
    "            and the redshift, Mpeak, colors, and \"weights\" of simulated\n",
    "            galaxies whose colors are compatible with those of observed\n",
    "            galaxies.\n",
    "\n",
    "            The observed sample include galaxies from SDSS\n",
    "            (SDSS+BOSS+eBOSS), DEEP2, and VIPERS.\n",
    "\n",
    "            The simulated sample was created by sampling the parameter of\n",
    "            GALAXPY using a LH.\n",
    "\n",
    "            The weights of simulated galaxies are related to the number\n",
    "            density of observed galaxies in the same region of the color\n",
    "            space.\n",
    "\n",
    "            You only have to care about the method load_structure. '''\n",
    "\n",
    "        def __init__(self):\n",
    "            self.arr_c = []\n",
    "            self.arr_z = []\n",
    "            self.arr_m = []\n",
    "            self.arr_w = []\n",
    "\n",
    "        def append(self, c, z, m, w):\n",
    "            self.arr_c.append(c)\n",
    "            self.arr_z.append(z)\n",
    "            self.arr_m.append(m)\n",
    "            self.arr_w.append(w)\n",
    "\n",
    "        def ndarray(self):\n",
    "            self.arr_c = np.concatenate(self.arr_c)\n",
    "            self.arr_z = np.concatenate(self.arr_z)\n",
    "            self.arr_m = np.concatenate(self.arr_m)\n",
    "            self.arr_w = np.concatenate(self.arr_w)\n",
    "\n",
    "        def save_struct(self, name):\n",
    "            np.save(name + 'c.npy', self.arr_c)\n",
    "            np.save(name + 'z.npy', self.arr_z)\n",
    "            np.save(name + 'm.npy', self.arr_m)\n",
    "            np.save(name + 'w.npy', self.arr_w)\n",
    "\n",
    "        def load_struct(self, name):\n",
    "            self.arr_c = np.load(name + 'c.npy')\n",
    "            self.arr_z = np.load(name + 'z.npy')\n",
    "            self.arr_m = np.load(name + 'm.npy')\n",
    "            self.arr_w = np.load(name + 'w.npy')\n",
    "\n",
    "        def duplicate_data(self, zrange):\n",
    "            aa = np.where((self.arr_w > 50)\n",
    "                          & (self.arr_z >= zrange[0])\n",
    "                          & (self.arr_z < zrange[1]))[0]\n",
    "            print(aa.shape)\n",
    "            cc = np.repeat(aa, self.arr_w[aa].astype(int))\n",
    "            self.arr_cn = self.arr_c[cc, :]\n",
    "            self.arr_zn = self.arr_z[cc]\n",
    "            self.arr_mn = self.arr_m[cc]\n",
    "\n",
    "\n",
    "    def read_curated_data():\n",
    "        run_path = path_program + 'runs/run_z3/'\n",
    "\n",
    "        sim_q = Curated_sample()  # simulated colors quenched galaxies\n",
    "        sim_s = Curated_sample()  # simulated colors star-forming galaxies\n",
    "        obs_q = Curated_sample()  # observed colors quenched galaxies\n",
    "        obs_s = Curated_sample()  # observed colors star-forming galaxies\n",
    "\n",
    "        obs_q.load_struct(run_path + 'str_obs_q')\n",
    "        obs_s.load_struct(run_path + 'str_obs_s')\n",
    "        sim_q.load_struct(run_path + 'str_sim_q')\n",
    "        sim_s.load_struct(run_path + 'str_sim_s')\n",
    "\n",
    "        print(sim_q.arr_c.shape)\n",
    "        print(sim_s.arr_c.shape)\n",
    "        print(obs_q.arr_c.shape)\n",
    "        print(obs_s.arr_c.shape)\n",
    "\n",
    "        return sim_q, sim_s, obs_q, obs_s\n",
    "\n",
    "\n",
    "    sim_q, sim_s, obs_q, obs_s = read_curated_data()\n",
    "\n",
    "    if sim_obs_combine:\n",
    "        train_datafile = 'GalaxPy'\n",
    "\n",
    "        # 2.0 ####### TRAIN USING SIMULATION, TEST OBSERVATION ####\n",
    "\n",
    "        Trainfiles = np.append(sim_q.arr_c, sim_s.arr_c, axis=0)\n",
    "        TrainZ = np.append(sim_q.arr_z, sim_s.arr_z, axis=0)\n",
    "\n",
    "        Trainfiles = np.delete(Trainfiles, (4), axis=1)  ## deleting z-Y\n",
    "\n",
    "        Testfiles = np.append(obs_q.arr_c, obs_s.arr_c, axis=0)\n",
    "        TestZ = np.append(obs_q.arr_z, obs_s.arr_z, axis=0)\n",
    "\n",
    "        TrainshuffleOrder = np.arange(Trainfiles.shape[0])\n",
    "        np.random.shuffle(TrainshuffleOrder)\n",
    "\n",
    "        Trainfiles = Trainfiles[TrainshuffleOrder]\n",
    "        TrainZ = TrainZ[TrainshuffleOrder]\n",
    "\n",
    "        TestshuffleOrder = np.arange(Testfiles.shape[0])\n",
    "        np.random.shuffle(TestshuffleOrder)\n",
    "\n",
    "        Testfiles = Testfiles[TestshuffleOrder]\n",
    "        TestZ = TestZ[TestshuffleOrder]\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[:num_test]  # color mag\n",
    "\n",
    "        y_train = TrainZ[:num_train]  # spec z\n",
    "        y_test = TrainZ[:num_test]  # spec z\n",
    "\n",
    "    else:\n",
    "        train_datafile = 'SDSS'\n",
    "        # 1.1 ####### SIMULATED: QUENCHED ONLY ############\n",
    "        # Trainfiles = sim_q.arr_c\n",
    "        # TrainZ = sim_q.arr_z\n",
    "\n",
    "        # 1.2 ### SIMULATED: QUENCHED + STAR FORMATION ####\n",
    "\n",
    "        # Trainfiles =np.append( sim_q.arr_c, sim_s.arr_c, axis = 0)\n",
    "        # TrainZ = np.append( sim_q.arr_z, sim_s.arr_z, axis = 0)\n",
    "\n",
    "        # 1.3 ####### OBSERVED: QUENCHED + STAR FORMATION ####\n",
    "\n",
    "        Trainfiles = np.append(obs_q.arr_c, obs_s.arr_c, axis=0)\n",
    "        TrainZ = np.append(obs_q.arr_z, obs_s.arr_z, axis=0)\n",
    "\n",
    "        TrainshuffleOrder = np.arange(Trainfiles.shape[0])\n",
    "        np.random.shuffle(TrainshuffleOrder)\n",
    "\n",
    "        Trainfiles = Trainfiles[TrainshuffleOrder]\n",
    "        TrainZ = TrainZ[TrainshuffleOrder]\n",
    "\n",
    "        # 1 #################################\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[num_train + 1: num_train + num_test]  # color mag\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[num_train + 1: num_train + num_test]  # color mag\n",
    "\n",
    "        y_train = TrainZ[:num_train]  # spec z\n",
    "        y_test = TrainZ[num_train + 1: num_train + num_test]  # spec z\n",
    "\n",
    "    ############## THINGS ARE SAME AFTER THIS ###########\n",
    "\n",
    "    ## rescaling xmax/xmin\n",
    "    xmax = np.max([np.max(X_train, axis=0), np.max(X_test, axis=0)], axis=0)\n",
    "    xmin = np.min([np.min(X_train, axis=0), np.min(X_test, axis=0)], axis=0)\n",
    "\n",
    "    X_train = (X_train - xmin) / (xmax - xmin)\n",
    "    X_test = (X_test - xmin) / (xmax - xmin)\n",
    "\n",
    "    #### RESCALING X_train, X_test NOT done yet -- (g-i), (r-i) ... and i mag -->> Color/Mag issue\n",
    "\n",
    "    ymax = np.max([y_train.max(), y_test.max()])\n",
    "    ymin = np.min([y_train.min(), y_test.min()])\n",
    "\n",
    "    y_train = (y_train - ymin) / (ymax - ymin)\n",
    "    y_test = (y_test - ymin) / (ymax - ymin)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, ymax, ymin, xmax, xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tensors):\n",
    "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
    "    Args:\n",
    "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
    "      `namedtuple` or combinations thereof.\n",
    "\n",
    "    Returns:\n",
    "      ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
    "        `EagerTensor`s replaced by Numpy `ndarray`s.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        return tf.contrib.framework.nest.pack_sequence_as(\n",
    "            tensors,\n",
    "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    return sess.run(tensors)\n",
    "\n",
    "\n",
    "def neural_network_mod():\n",
    "    \"\"\"\n",
    "    loc, scale, logits = NN(x; theta)\n",
    "\n",
    "    Args:\n",
    "      X: Input Tensor containing input data for the MDN\n",
    "    Returns:\n",
    "      locs: The means of the normal distributions that our data is divided into.\n",
    "      scales: The scales of the normal distributions that our data is divided\n",
    "        into.\n",
    "      logits: The probabilities of ou categorical distribution that decides\n",
    "        which normal distribution our data points most probably belong to.\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float64,name='X',shape=(None,D))\n",
    "    # 2 hidden layers with 15 hidden units\n",
    "    net = tf.layers.dense(X, 32, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 16, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 8, activation=tf.nn.relu)\n",
    "    locs = tf.layers.dense(net, K, activation=None)\n",
    "    scales = tf.layers.dense(net, K, activation=tf.exp)\n",
    "    logits = tf.layers.dense(net, K, activation=None)\n",
    "    outdict= {'locs':locs, 'scales':scales, 'logits':logits}\n",
    "    hub.add_signature(inputs=X,outputs=outdict)\n",
    "\n",
    "    return locs, scales, logits\n",
    "\n",
    "\n",
    "def mixture_model(X,Y,learning_rate=1e-3,decay_rate=.95,step=1000,train=True):\n",
    "    if train:\n",
    "        dict = neural_network(tf.convert_to_tensor(X),as_dict=True)\n",
    "    else:\n",
    "        dict = neural_network_t(tf.convert_to_tensor(X),as_dict=True)\n",
    "    locs = dict['locs'] ; scales = dict['scales'] ; logits = dict['logits']\n",
    "    cat = tfd.Categorical(logits=logits)\n",
    "    components = [tfd.Normal(loc=loc, scale=scale) for loc, scale\n",
    "                  in zip(tf.unstack(tf.transpose(locs)),\n",
    "                         tf.unstack(tf.transpose(scales)))]\n",
    "\n",
    "    y = tfd.Mixture(cat=cat, components=components)\n",
    "    #define loss function\n",
    "    log_likelihood = y.log_prob(Y)\n",
    "    # log_likelihood = -tf.reduce_sum(log_likelihood/(1. + y_train)**2 )\n",
    "    y_mean = np.median(Y)\n",
    "    log_likelihood = -tf.reduce_sum(log_likelihood)\n",
    "    #log_likelihood = -tf.reduce_sum(log_likelihood*(y_mean-y_train)**4 )\n",
    "    if train:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        decayed_lr = tf.train.exponential_decay(learning_rate,\n",
    "                                        global_step, step,\n",
    "                                        decay_rate, staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(decayed_lr)\n",
    "        train_op = optimizer.minimize(log_likelihood)\n",
    "        evaluate(tf.global_variables_initializer())\n",
    "        return log_likelihood, train_op, logits, locs, scales\n",
    "    else:\n",
    "        evaluate(tf.global_variables_initializer())\n",
    "        return log_likelihood, logits, locs, scales\n",
    "\n",
    "def train(log_likelihood,train_op,n_epoch):\n",
    "    train_loss = np.zeros(n_epoch)\n",
    "    test_loss = np.zeros(n_epoch)\n",
    "    for i in range(n_epoch):\n",
    "        _, loss_value = evaluate([train_op, log_likelihood])\n",
    "        train_loss[i] = loss_value\n",
    "    plt.plot(np.arange(n_epoch), -train_loss / len(X_train), label='Train Loss')\n",
    "    # plt.savefig('../Plots/T_loss_function.pdf')\n",
    "    return train_loss\n",
    "\n",
    "def get_predictions(logits,locs,scales):\n",
    "    pred_weights, pred_means, pred_std = evaluate([tf.nn.softmax(logits), locs, scales])\n",
    "    return pred_weights, pred_means, pred_std\n",
    "\n",
    "def testing(X_test,y_test):\n",
    "\n",
    "    log_likelihood,  logits, locs, scales = mixture_model(X_test,y_test,train=False)\n",
    "    #_, loss_value = evaluate([train_op, log_likelihood])\n",
    "    pred_weights, pred_means, pred_std = get_predictions(logits,locs,scales)\n",
    "    return pred_weights, pred_means, pred_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal_mix(pis, mus, sigmas, ax, label='', comp=True):\n",
    "  \"\"\"Plots the mixture of Normal models to axis=ax comp=True plots all\n",
    "  components of mixture model\n",
    "  \"\"\"\n",
    "  # x = np.linspace(-10.5, 10.5, 250)\n",
    "  x = np.linspace(-0.1, 1.1, 250)\n",
    "  final = np.zeros_like(x)\n",
    "  for i, (weight_mix, mu_mix, sigma_mix) in enumerate(zip(pis, mus, sigmas)):\n",
    "    temp = stats.norm.pdf(x, mu_mix, sigma_mix) * weight_mix\n",
    "    final = final + temp\n",
    "    if comp:\n",
    "      ax.plot(x, temp, label='Normal ' + str(i), alpha =0.3)\n",
    "  ax.plot(x, final, label='Mixture of Normals ' + label)\n",
    "  ax.legend(fontsize=13)\n",
    "  return final\n",
    "\n",
    "def plot_pdfs(pred_means,pred_weights,pred_std, y,num=4, train=True):\n",
    "    np.random.seed(12)\n",
    "#     np.random.seed(12)\n",
    "\n",
    "#     if train:\n",
    "#         obj = [random.randint(0,num_train-1) for x in range(num)]\n",
    "#     else:\n",
    "#         obj = [random.randint(0,num_test-1) for x in range(num)]\n",
    "#     #obj = [93, 402, 120,789,231,4,985]\n",
    "    if train:\n",
    "        obj = np.random.randint(0,num_train-1,num)\n",
    "    else:\n",
    "        obj = np.random.randint(0,num_test-1,num)\n",
    "    #obj = [93, 402, 120,789,231,4,985]\n",
    "    \n",
    "    print(obj)\n",
    "#     fig, axes = plt.subplots(nrows=num, ncols=1, sharex = True, figsize=(8, 7), num='PDFs')\n",
    "    allfs = []\n",
    "    for i in range(len(obj)):\n",
    "        fs = plot_normal_mix(pred_weights[obj][i], pred_means[obj][i],\n",
    "                    pred_std[obj][i], axes[i], comp=False)\n",
    "        allfs.append(fs)\n",
    "        axes[i].axvline(x=y[obj][i], color='black', alpha=0.5)\n",
    "#         axes[i].text(0.3, 4.0, 'ID: ' +str(obj[i]), horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "#     plt.xlabel(r' rescaled[$z_{pred}]$', fontsize = 19)\n",
    "    plt.xlabel(r'$z_{\\rm pred}]$', fontsize = 22)\n",
    "    plt.ylabel('$PDF$', fontsize = 22)\n",
    "\n",
    "\n",
    "    # plt.savefig('../Plots/T_pdfs.pdf')\n",
    "#     plt.show()\n",
    "\n",
    "def plot_pred_mean(pred_means,pred_weights,pred_std,ymax,ymin,y,select='no'):\n",
    "    y_pred = np.sum(pred_means*pred_weights, axis = 1)\n",
    "    y_pred_std = np.sum(pred_std*pred_weights, axis = 1)\n",
    "\n",
    "    plt.figure(22, figsize=(9,8))\n",
    "\n",
    "    #ymax=1\n",
    "    #ymin=0\n",
    "    # if select == 'yes':\n",
    "    #     y_pred = y_pred[obj]\n",
    "    #     y_train = y_train[obj]\n",
    "    #     y_pred_std = y_pred_std[obj]\n",
    "\n",
    "    # plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "\n",
    "    # plt.errorbar( (ymax - ymin)*(y_train)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "    plt.errorbar( (ymax - ymin)*(y)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(y_pred_std), fmt='o', ms = 2, alpha = 0.1)\n",
    "\n",
    "    #switched\n",
    "    #plt.errorbar(  (ymax - ymin)*(y_pred)+ymin, (ymax - ymin)*(y_train)+ymin, yerr= (ymax - ymin)*(y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "    #plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "    plt.plot((ymax - ymin)*(y)+ymin, (ymax - ymin)*( y)+ymin, 'k')\n",
    "\n",
    "    plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "    plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "    #plt.xlim([0,1])\n",
    "    #plt.ylim([0,1])\n",
    "    plt.title('weight x mean')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('../Plots/T_pred_mean.pdf')\n",
    "#     plt.show()\n",
    "\n",
    "def plot_pred_peak(pred_means,pred_weights,pred_std,ymax,ymin,y,select='no'):\n",
    "    def peak(weight,sigma):\n",
    "        return weight/np.sqrt(2*np.pi*sigma**2)\n",
    "\n",
    "    peak_max = np.argmax(peak(pred_weights,pred_std),axis=1)\n",
    "    y_pred = np.array([pred_means[i,peak_max[i]] for i in range(len(y))])\n",
    "    y_pred_std = np.array([pred_std[i,peak_max[i]] for i in range(len(y))])\n",
    "    plt.figure(24, figsize=(9, 8))\n",
    "    # if select == 'yes':\n",
    "    #     y_pred = y_pred[obj]\n",
    "    #     y_train = y_train[obj]\n",
    "    #     y_pred_std = y_pred_std[obj]\n",
    "    # plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "    # plt.errorbar((ymax - ymin)*(y_train)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(\n",
    "    #   y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "    plt.errorbar((ymax - ymin)*(y)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(\n",
    "      y_pred_std), fmt='o', ms = 2, alpha = 0.1)\n",
    "\n",
    "\n",
    "    #plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "    plt.plot((ymax - ymin)*(y_test)+ymin, (ymax - ymin)*(y_test)+ymin, 'k')\n",
    "    plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "    plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "    #plt.xlim([0,1])\n",
    "    #plt.ylim([0,1])\n",
    "    plt.title('highest peak')\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "def plot_pred_weight(pred_means,pred_weights,pred_std,ymax,ymin,y,select='no'):\n",
    "    weight_max = np.argmax(pred_weights, axis = 1)  ## argmax or max???\n",
    "\n",
    "    y_pred = np.array([pred_means[i,weight_max[i]] for i in range(len(y))])\n",
    "    y_pred_std = np.array([pred_std[i,weight_max[i]] for i in range(len(y))])\n",
    "\n",
    "    plt.figure(29, figsize=(9, 8))\n",
    "    # if select == 'yes':\n",
    "    #     y_pred = y_pred[obj]\n",
    "    #     y_train = y_train[obj]\n",
    "    #     y_pred_std = y_pred_std[obj]\n",
    "\n",
    "    # plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "    # plt.errorbar((ymax - ymin)*(y_train)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(\n",
    "    #   y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "    plt.errorbar((ymax - ymin)*(y)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(\n",
    "      y_pred_std), fmt='o', ms = 2, alpha = 0.1)\n",
    "\n",
    "    #plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "    plt.plot((ymax - ymin)*(y_test)+ymin, (ymax - ymin)*(y_test)+ymin, 'k')\n",
    "    plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "    plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "    #plt.xlim([0,1])\n",
    "    #plt.ylim([0,1])\n",
    "    plt.title('highest weight')\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "def per_stats(pred_means,pred_weights,pred_std,ymax,ymin,y):\n",
    "    y_pred = np.sum(pred_means*pred_weights, axis = 1)\n",
    "    y_pred_std = np.sum(pred_std*pred_weights, axis = 1)\n",
    "    y_pred = (ymax - ymin)*(y_pred)+ymin\n",
    "    y_pred_std = (ymax - ymin)*(y_pred_std)\n",
    "    y = (ymax - ymin)*(y)+ymin\n",
    "    diff = y_pred-y\n",
    "    mean_diff = np.mean(diff)\n",
    "    med_diff = np.median(diff)\n",
    "    std_diff = np.std(diff)\n",
    "    mean_sigma = np.mean(y_pred_std)\n",
    "    med_sigma = np.median(y_pred_std)\n",
    "    std_sigma = np.std(y_pred_std)\n",
    "    return mean_diff, med_diff, std_diff, mean_sigma, med_sigma, std_sigma\n",
    "\n",
    "def plot_cum_sigma(pred_weights,pred_std,ymax,ymin):\n",
    "    #y_pred_std = np.sum(pred_std*pred_weights, axis = 1)\n",
    "\n",
    "    weight_max = np.argmax(pred_weights, axis = 1)  ## argmax or max???\n",
    "    y_pred_std = np.array( [pred_std[i,weight_max[i] ] for i in range(len(pred_weights[0]))])\n",
    "    y_pred_std = (ymax - ymin)*(y_pred_std)\n",
    "    plt.figure(222)\n",
    "    plt.hist(y_pred_std,1000, density=True, histtype='step', cumulative=True)\n",
    "    plt.xlabel('Sigma')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# #########################################################\n",
    "\n",
    "# n_epochs = 100000 #1000 #20000 #20000\n",
    "# K = 3 \n",
    "# learning_rate = 5e-3\n",
    "# decay_rate= 0.0\n",
    "# step=100\n",
    "# num_train = 800000\n",
    "\n",
    "# syntheticTrain = False \n",
    "\n",
    "# save_mod_2 = '../'+'hub_mod_Synthetic_'+str(syntheticTrain)+'_lr_'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)\n",
    "\n",
    "# # save_mod_2 = '../hub_mod_Synthetic_False_lr_0.005_dr0.01_step100_ne100000_k3_nt800000'\n",
    "# #########################################################\n",
    "# #########################################################\n",
    "\n",
    "\n",
    "\n",
    "# #### 12 million run!!!\n",
    "# n_epochs = 100000 #1000 #20000 #20000\n",
    "# # N = 4000  # number of data points  -- replaced by num_trai\n",
    "# D = 5 #6  # number of features  (8 for DES, 6 for COSMOS)\n",
    "# K = 3 # number of mixture components\n",
    "\n",
    "\n",
    "# learning_rate = 5e-3\n",
    "# decay_rate= 0.0\n",
    "\n",
    "# step=100\n",
    "\n",
    "\n",
    "# num_train = 12000000 #800000\n",
    "# num_test = 1000 #params.num_test # 32\n",
    "\n",
    "\n",
    "# syntheticTrain = True # True # (sim_obs_combine) True -- train using GalaxyPy, False -- train using\n",
    "\n",
    "# save_mod = '../'+'hub_mod_Synthetic_'+str(syntheticTrain)+'_lr_'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 1000\n",
    "D = 5 #6  # number of features  (8 for DES, 6 for COSMOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### -- synthetic False works --- ######\n",
    "\n",
    "# n_epochs = 1000 #1000 #20000 #20000\n",
    "# # N = 4000  # number of data points  -- replaced by num_trai\n",
    "# D = 5 #6  # number of features  (8 for DES, 6 for COSMOS)\n",
    "# K = 3 # number of mixture components\n",
    "\n",
    "# learning_rate = 5e-3\n",
    "# decay_rate= 0.0\n",
    "# step=100\n",
    "\n",
    "# num_train = 100000 #800000\n",
    "# # num_test = 2000 #10000 #params.num_test # 32\n",
    "\n",
    "# # save_mod = '../'+'hub_mod_lr_1'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)\n",
    "\n",
    "# ##### MODEL 2 ####\n",
    "\n",
    "# save_mod_2 = '../' + 'hub_mod_lr_2'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hub_mod_Synthetic_False_lr_0.0001_dr0.01_step100_ne20000_k3_nt800000\n",
    "# XXXX hub_mod_Synthetic_False_lr_0.001_dr0.05_step100_ne10000_k3_nt800000\n",
    "# XXXX hub_mod_Synthetic_False_lr_0.005_dr0.01_step100_ne100000_k3_nt800000 \n",
    "# XXXX hub_mod_Synthetic_False_lr_0.005_dr0.0_step100_ne100000_k3_nt800000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "\n",
    "n_epochs = 20000 #1000 #20000 #20000\n",
    "K = 3 \n",
    "learning_rate = 1e-4\n",
    "decay_rate= 0.01\n",
    "step=100\n",
    "num_train = 800000\n",
    "\n",
    "syntheticTrain = False \n",
    "\n",
    "save_mod_2 = '../saved_hubs/'+'hub_mod_Synthetic_'+str(syntheticTrain)+'_lr_'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 12 million run!!!\n",
    "n_epochs = 100000 #1000 #20000 #20000\n",
    "K = 3 # number of mixture components\n",
    "learning_rate = 5e-3\n",
    "decay_rate= 0.0\n",
    "step=100\n",
    "\n",
    "num_train = 12000000 #800000\n",
    "\n",
    "syntheticTrain = True # True # (sim_obs_combine) True -- train using GalaxyPy, False -- train using\n",
    "\n",
    "save_mod = '../saved_hubs/'+'hub_mod_Synthetic_'+str(syntheticTrain)+'_lr_'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../saved_hubs/hub_mod_Synthetic_True_lr_0.005_dr0.0_step50_ne100000_k3_nt12000000'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_mod# --- synthetic 1 mil run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../saved_hubs/hub_mod_Synthetic_False_lr_0.0001_dr0.01_step100_ne20000_k3_nt800000'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_mod_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n_epochs = 10000 #100000 #1000 #20000 #20000\n",
    "# # N = 4000  # number of data points  -- replaced by num_trai\n",
    "# D = 5 #6  # number of features  (8 for DES, 6 for COSMOS)\n",
    "# K = 3 # number of mixture components\n",
    "\n",
    "\n",
    "# learning_rate = 1e-3 #5e-3\n",
    "# decay_rate= 0.05 #0.0\n",
    "# step=100\n",
    "\n",
    "\n",
    "# num_train = 800000 #12000000 #800000\n",
    "# num_test = 5000 #params.num_test # 32\n",
    "\n",
    "\n",
    "# syntheticTrain = True # True # (sim_obs_combine) True -- train using GalaxyPy, False -- train using\n",
    "\n",
    "# save_mod = '../' + 'hub_mod_Synthetic_'+str(syntheticTrain)+'_lr_'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)\n",
    "\n",
    "# syntheticTrain = False # True # (sim_obs_combine) True -- train using GalaxyPy, False -- train using\n",
    "\n",
    "# save_mod = '../' + 'hub_mod_Synthetic_'+str(syntheticTrain)+'_lr_'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6304534, 6)\n",
      "(7237871, 6)\n",
      "(456685, 5)\n",
      "(423353, 5)\n",
      "Size of features in test data: (1000, 5)\n",
      "Size of output in test data: (1000,)\n"
     ]
    }
   ],
   "source": [
    "############ loading data ############\n",
    "\n",
    "_, _, X_test, y_test, ymax, ymin, xmax, xmin = ReadGalaxPy(path_program = '../../../Data/fromGalaxev/photozs/datasets/', sim_obs_combine = True)\n",
    "\n",
    "# print(\"Size of features in training data: {}\".format(X_train.shape))\n",
    "# print(\"Size of output in training data: {}\".format(y_train.shape))\n",
    "print(\"Size of features in test data: {}\".format(X_test.shape))\n",
    "print(\"Size of output in test data: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UnsupportedHandleError",
     "evalue": "unsupported handle format '../saved_hubs/hub_mod_Synthetic_True_lr_0.005_dr0.0_step50_ne100000_k3_nt12000000'. No resolvers found that can successfully resolve it. If the handle points to the local filesystem, the error indicates that the module directory does not exist. Supported handle formats: URLs pointing to a TGZ  file (e.g. https://address/module.tgz), or Local File System directory file (e.g. /tmp/my_local_module).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedHandleError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-cf13c0ed13da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##load network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mneural_network_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m##testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \"\"\"\n\u001b[1;32m    153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_module_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36mas_module_spec\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_module_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown module spec type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36mload_module_spec\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mon\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mhandling\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \"\"\"\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     raise RuntimeError(\n\u001b[1;32m     44\u001b[0m         \"Missing implementation that supports: %s(*%r, **%r)\" % (\n",
      "\u001b[0;32m~/anaconda3/envs/env_py37/lib/python3.7/site-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;34m\"exist. Supported handle formats: URLs pointing to a TGZ  file \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;34m\"(e.g. https://address/module.tgz), or Local File System directory \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \"file (e.g. /tmp/my_local_module).\" % handle)\n\u001b[0m",
      "\u001b[0;31mUnsupportedHandleError\u001b[0m: unsupported handle format '../saved_hubs/hub_mod_Synthetic_True_lr_0.005_dr0.0_step50_ne100000_k3_nt12000000'. No resolvers found that can successfully resolve it. If the handle points to the local filesystem, the error indicates that the module directory does not exist. Supported handle formats: URLs pointing to a TGZ  file (e.g. https://address/module.tgz), or Local File System directory file (e.g. /tmp/my_local_module)."
     ]
    }
   ],
   "source": [
    "#### MODEL 1 ####\n",
    "\n",
    "##load network\n",
    "neural_network_t = hub.Module(save_mod)\n",
    "\n",
    "##testing\n",
    "test_weights, test_means, test_std = testing(X_test,y_test)\n",
    "\n",
    "##plotting\n",
    "plot_pred_mean(test_means,test_weights,test_std,ymax,ymin,y_test)\n",
    "test_mean_diff, test_med_diff, test_std_diff, test_mean_sigma, test_med_sigma, test_std_sigma = per_stats(test_means,test_weights,test_std,ymax,ymin,y_test)\n",
    "plot_pred_peak(test_means,test_weights,test_std,ymax,ymin,y_test)\n",
    "plot_pred_weight(test_means,test_weights,test_std,ymax,ymin,y_test)\n",
    "plot_cum_sigma(test_weights,test_std,ymax,ymin)\n",
    "\n",
    "\n",
    "##### MODEL 2 ####\n",
    "\n",
    "##load network\n",
    "neural_network_t = hub.Module(save_mod_2)\n",
    "\n",
    "##testing\n",
    "test_weights_2, test_means_2, test_std_2 = testing(X_test,y_test)\n",
    "\n",
    "##plotting\n",
    "plot_pred_mean(test_means_2,test_weights_2,test_std_2,ymax,ymin,y_test)\n",
    "test_mean_diff_2, test_med_diff_2, test_std_diff_2, test_mean_sigma_2, test_med_sigma_2, test_std_sigma_2 = per_stats(test_means_2,test_weights_2,test_std_2,ymax,ymin,y_test)\n",
    "plot_pred_peak(test_means_2,test_weights_2,test_std_2,ymax,ymin,y_test)\n",
    "plot_pred_weight(test_means_2,test_weights_2,test_std_2,ymax,ymin,y_test)\n",
    "plot_cum_sigma(test_weights_2,test_std_2,ymax,ymin)\n",
    "\n",
    "## plotting pdfs\n",
    "nrows = 3\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=1, sharex = True, figsize=(9, nrows*3), num='pdfs')\n",
    "plot_pdfs(test_means,test_weights,test_std, y_test, num=nrows, train=False)\n",
    "plot_pdfs(test_means_2,test_weights_2,test_std_2, y_test, num=nrows, train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal_mix(pis, mus, sigmas, ax, label='', color = '', comp=True):\n",
    "  \"\"\"Plots the mixture of Normal models to axis=ax comp=True plots all\n",
    "  components of mixture model\n",
    "  \"\"\"\n",
    "\n",
    "  x = np.linspace(-0.1, 1.1, 250)\n",
    "  final = np.zeros_like(x)\n",
    "  for i, (weight_mix, mu_mix, sigma_mix) in enumerate(zip(pis, mus, sigmas)):\n",
    "    temp = stats.norm.pdf(x, mu_mix, sigma_mix) * weight_mix\n",
    "    final = final + temp\n",
    "    if comp:\n",
    "#       ax.plot(x, temp, label='Normal ' + str(i), alpha =0.6)\n",
    "      ax.plot(x, temp, 'k--', alpha =0.9)\n",
    "\n",
    "#       ax.plot(x, temp/final.max(), alpha =0.5)\n",
    "\n",
    "  ax.plot(x, final,label=label, color = color)\n",
    "#   ax.plot(x, final/final.max(), label=label, color = color)\n",
    "\n",
    "    \n",
    "  ax.legend(fontsize=13)\n",
    "  return final\n",
    "\n",
    "def plot_pdfs(pred_means,pred_weights,pred_std, y,num=4, label = '', color = '', train=False, comp = False):\n",
    "    np.random.seed(132)\n",
    "\n",
    "    if train:\n",
    "        obj = np.random.randint(0,num_train-1,num)\n",
    "    else:\n",
    "        obj = np.random.randint(0,num_test-1,num)\n",
    "#     obj = [462, 667, 81]\n",
    "#     obj = [462, 102, 81]\n",
    "    obj = [462, 18, 81]\n",
    "    \n",
    "    print(obj)\n",
    "\n",
    "    allfs = []\n",
    "    for i in range(len(obj)):\n",
    "        print(i)\n",
    "        if (i==0):\n",
    "            fs = plot_normal_mix(pred_weights[obj][i], pred_means[obj][i], pred_std[obj][i], axes[i], label = label, color = color, comp=comp)\n",
    "        else: fs = plot_normal_mix(pred_weights[obj][i], pred_means[obj][i], pred_std[obj][i], axes[i], label = '', color = color, comp=comp)\n",
    "\n",
    "        axes[i].set_ylabel(r'${\\rm PDF}$', fontsize = 22)\n",
    "        allfs.append(fs)\n",
    "        axes[i].axvline(x=y[obj][i], color='black', alpha=0.5)\n",
    "        \n",
    "\n",
    "    plt.xlabel('Photometric redshift', fontsize = 26)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## plotting pdfs\n",
    "nrows = 3\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=1, sharex = True, figsize=(9, nrows*3), num='pdfs')\n",
    "plot_pdfs(test_means,test_weights,test_std, y_test, num=nrows, label = 'Training with synthetic data', color = 'red', train=False)\n",
    "plot_pdfs(test_means_2,test_weights_2,test_std_2, y_test, num=nrows, label = 'Training with observational data', color = 'blue', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_means' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d733116740ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#### cumulative --> y_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_means\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0my_pred_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_std\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_means' is not defined"
     ]
    }
   ],
   "source": [
    "y_test2 = y_test ### CHECK THIS AGAIN -- THERE MAY BE ISSUES WITH MIXING TRAINING AND TESTING SETS\n",
    "ymax2 = ymax\n",
    "ymin2 = ymin\n",
    "\n",
    "\n",
    "#### choosing y_pred here -- may not be a good choice\n",
    "#### cumulative --> y_pred\n",
    "\n",
    "y_pred = np.sum(test_means*test_weights, axis = 1)\n",
    "y_pred_std = np.sum(test_std*test_weights, axis = 1)\n",
    "\n",
    "y_pred_2 = np.sum(test_means_2*test_weights_2, axis = 1)\n",
    "y_pred_std_2 = np.sum(test_std_2*test_weights_2, axis = 1)\n",
    "\n",
    "\n",
    "# #### highest weight --> y_pred\n",
    "\n",
    "# weight_max = np.argmax(test_weights, axis = 1) \n",
    "\n",
    "# y_pred = np.array([test_means[i,weight_max[i]] for i in range(len(y_test))])\n",
    "# y_pred_std = np.array([test_std[i,weight_max[i]] for i in range(len(y_test))])\n",
    "\n",
    "\n",
    "\n",
    "# weight_max_2 = np.argmax(test_weights_2, axis = 1) \n",
    "\n",
    "# y_pred_2 = np.array([test_means_2[i,weight_max_2[i]] for i in range(len(y_test2))])\n",
    "# y_pred_std_2 = np.array([test_std_2[i,weight_max_2[i]] for i in range(len(y_test2))])\n",
    "\n",
    "\n",
    "### highest peak --> y_pred\n",
    "\n",
    "# def peak(weight,sigma):\n",
    "#     return weight/np.sqrt(2*np.pi*sigma**2)\n",
    "\n",
    "# peak_max = np.argmax(peak(test_weights,test_std),axis=1)\n",
    "# y_pred = np.array([test_means[i,peak_max[i]] for i in range(len(y_test))])\n",
    "# y_pred_std = np.array([test_std[i,peak_max[i]] for i in range(len(y_test))])\n",
    "\n",
    "# peak_max_2 = np.argmax(peak(test_weights_2,test_std_2),axis=1)\n",
    "# y_pred_2 = np.array([test_means_2[i,peak_max_2[i]] for i in range(len(y_test2))])\n",
    "# y_pred_std_2 = np.array([test_std_2[i,peak_max_2[i]] for i in range(len(y_test2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-517eb96e523a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msigmaNMAD_obs_all\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msigmaNMAD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mymax2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mymin2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymin2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mymax2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mymin2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymin2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_pred_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msigmaNMAD_combine_all\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msigmaNMAD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mymax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mymax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_2' is not defined"
     ]
    }
   ],
   "source": [
    "def sigmaNMAD(z_spec, z_pho):\n",
    "    return 1.48*np.median( np.abs( z_pho - z_spec)/(1 + z_spec))\n",
    "    # else: return 1.48*np.median( np.abs( z_pho - z_spec)/(1 + z_spec),)\n",
    "\n",
    "\n",
    "\n",
    "def outlierFrac(z_spec, z_pho, threshold = 0.15):\n",
    "    outliers = z_pho[ (np.abs(z_spec - z_pho)) >= threshold*z_spec ]\n",
    "    return 100.0*len(outliers)/np.shape(z_pho)[0]\n",
    "\n",
    "\n",
    "sigmaNMAD_obs_all =  sigmaNMAD( (ymax2 - ymin2)*(ymin2 + y_test2), (ymax2 - ymin2)*(ymin2 + y_pred_2) )\n",
    "\n",
    "sigmaNMAD_combine_all =  sigmaNMAD( (ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred) )\n",
    "\n",
    "\n",
    "print( 'sigmaNMAD_combine_all ', sigmaNMAD_combine_all  ) ## combine\n",
    "print( 'sigmaNMAD_obs_all', sigmaNMAD_obs_all ) # obs only\n",
    "\n",
    "\n",
    "outFr_obs_all = outlierFrac( (ymax2 - ymin2)*(ymin2 + y_test2), (ymax2 - ymin2)*(ymin2 + y_pred_2), 0.15 )\n",
    "\n",
    "outFr_combine_all =  outlierFrac( (ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred) , 0.15)\n",
    "\n",
    "\n",
    "print( 'outFr_combine_all ', outFr_combine_all  ) ## combine\n",
    "print( 'outFr_obs_all', outFr_obs_all ) # obs only\n",
    "\n",
    "\n",
    "sigmaNMAD_obs_all =  sigmaNMAD( (ymax2 - ymin2)*(ymin2 + y_test2), (ymax2 - ymin2)*(ymin2 +\n",
    "                                                                                      y_pred_2) )\n",
    "\n",
    "sigmaNMAD_combine_all =  sigmaNMAD( (ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin +\n",
    "                                                                                      y_pred) )\n",
    "\n",
    "\n",
    "print( 'sigmaNMAD_combine_all ', sigmaNMAD_combine_all  ) #\n",
    "# combine\n",
    "print( 'sigmaNMAD_obs_all', sigmaNMAD_obs_all ) # obs only\n",
    "####################### model 2 ends ###########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(22)\n",
    "\n",
    "\n",
    "## Overall mean --- weight * mean\n",
    "ifPlotWeighted = True\n",
    "\n",
    "if ifPlotWeighted:\n",
    "    plt.figure(22, figsize=(10, 10))\n",
    "\n",
    "#     y_pred = np.sum(pred_means_new*pred_weights_new, axis = 1)\n",
    "#     y_pred_std_new = np.sum(pred_std_new*pred_weights_new, axis = 1)\n",
    "\n",
    "    # # plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "#     plt.errorbar((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(\n",
    "#       ymin + y_pred_std_new), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "    \n",
    "    plt.errorbar((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(\n",
    "      ymin + y_pred_std), fmt='ro', ecolor='r', ms = 5, alpha = 0.3, label = 'Training with synthetic data')\n",
    "\n",
    "#     plt.errorbar((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(\n",
    "#       ymin + y_pred_std), fmt='wo', ecolor='w', ms = 5, alpha = 0.8, label = 'Training with synthetic data')\n",
    "\n",
    "\n",
    "\n",
    "    # plt.text(0.8, 2.0, datafile, horizontalalignment='center', verticalalignment='center')\n",
    "    # plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "    # plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "\n",
    "    # plt.title('weight x mean')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "## Overall mean --- weight * mean\n",
    "# ifPlotWeighted = True\n",
    "\n",
    "\n",
    "if ifPlotWeighted:\n",
    "\n",
    "#     y_pred_new2 = np.sum(pred_means_new2*pred_weights_new2, axis = 1)\n",
    "#     y_pred_std_new2 = np.sum(pred_std_new2*pred_weights_new2, axis = 1)\n",
    "\n",
    "#     plt.figure(22, figsize=(6,6))\n",
    "\n",
    "\n",
    "    # plt.scatter(y_test2, y_pred, facecolors='k', s = 1)\n",
    "#     plt.errorbar((ymax2 - ymin2)*(ymin2 + y_test2), (ymax2 - ymin2)*(ymin2 + y_pred), yerr= (ymax2 - ymin2)*(\n",
    "#       ymin2 + y_pred_std_new2), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "    \n",
    "    plt.errorbar((ymax2 - ymin2)*(ymin2 + y_test2), (ymax2 - ymin2)*(ymin2 + y_pred_2), yerr= (ymax2 - ymin2)*(\n",
    "      ymin2 + y_pred_std_2), fmt='bo', ecolor='b', ms = 5, alpha = 0.3, label = 'Training with observational data')\n",
    "\n",
    "#     plt.errorbar((ymax2 - ymin2)*(ymin2 + y_test2), (ymax2 - ymin2)*(ymin2 + y_pred_2), yerr= (ymax2 - ymin2)*(\n",
    "#       ymin2 + y_pred_std_2), fmt='co', ecolor='c', ms = 5, alpha = 0.8, label = 'Training with observational data')\n",
    "\n",
    "\n",
    "\n",
    "    # plt.text(0.8, 2.0, datafile, horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# # plt.text(0.1, 0.9, r'$\\sigma_{NMAD}$ = %.3f'%sigmaNMAD_obs_all, color = 'red' , size = 20)\n",
    "# # plt.text(0.1, 0.85, r'$\\sigma_{NMAD}$ = %.3f'%sigmaNMAD_combine_all, color = 'blue' , size = 20)\n",
    "# plt.text(0.6, 0.2, r'$\\sigma_{NMAD}$ = %.3f'%sigmaNMAD_obs_all, color = 'blue' , size = 20)\n",
    "# plt.text(0.6, 0.1, r'$\\sigma_{NMAD}$ = %.3f'%sigmaNMAD_combine_all, color = 'red' , size = 20)\n",
    "\n",
    "# plt.text(0.1, 0.9, r'GalaxPy training', color = 'red' , size = 20)\n",
    "# plt.text(0.1, 0.85, r'SDSS training', color = 'blue' , size = 20)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k')\n",
    "plt.plot([0, 1], 0.85*np.array([0, 1]), 'k-.')\n",
    "plt.plot([0, 1], 1.15*np.array([0, 1]), 'k-.')\n",
    "\n",
    "# plt.ylabel(r'$z_{\\rm phot}$', fontsize=30)\n",
    "# plt.xlabel(r'$z_{\\rm spec}$', fontsize=30)\n",
    "\n",
    "\n",
    "plt.ylabel(r'Photometric redshift', fontsize=25)\n",
    "plt.xlabel(r'True redshift', fontsize=25)\n",
    "plt.xlim(0.0, 1)\n",
    "plt.ylim(0.0, 1)\n",
    "\n",
    "# plt.legend(fontsize = 'large', markerscale=3., numpoints=3)\n",
    "# plt.title('weight x mean')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "\n",
    "leg = plt.legend(fontsize = 'xx-large', markerscale=1., numpoints=2)\n",
    "\n",
    "for artist, text in zip(leg.legendHandles, leg.get_texts()):\n",
    "    col = artist.get_color()\n",
    "    if isinstance(col, np.ndarray):\n",
    "        col = col[0]\n",
    "    text.set_color(col)\n",
    "    text.set_alpha(1.0)\n",
    "\n",
    "\n",
    "plt.savefig('phoz_compare.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(472, figsize = (16, 10))\n",
    "nbins = 50\n",
    "plt.hist((ymax - ymin)*(ymin + y_test), bins = nbins,  label = 'Observed distribution', color='k', histtype='stepfilled', alpha = 0.2, lw = 2, normed= True)\n",
    "plt.hist((ymax - ymin)*(ymin + y_pred), bins = nbins,  label = 'Training with synthetic data', color = 'r', histtype='stepfilled', lw = 2, alpha = 0.2, normed= True)\n",
    "plt.hist((ymax2 - ymin2)*(ymin2 + y_pred_2), bins = nbins,  label = 'Training with observational data', color = 'b', histtype='stepfilled', lw = 2, alpha = 0.2, normed= True)\n",
    "plt.legend( fontsize = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.Series((ymax - ymin)*(ymin + y_test)).add_prefix('Observed distribution')\n",
    "s1 = pd.Series((ymax - ymin)*(ymin + y_pred)).add_prefix('Training with synthetic data')\n",
    "s2 = pd.Series((ymax2 - ymin2)*(ymin2 + y_pred_2)).add_prefix('Training with observational data')\n",
    "\n",
    "plt.figure(12, figsize = (12, 8))\n",
    "\n",
    "ax = s.plot.hist(alpha=0.5, density=10, label = 'Observed distribution')\n",
    "ax = s.plot.kde(linewidth = 2, secondary_y=True, label = 'KDE fit - Observed distribution')\n",
    "ax = s1.plot.hist(alpha=0.5, density=10, label = 'Training with synthetic data')\n",
    "ax = s1.plot.kde(linewidth = 2,  secondary_y=True, linestyle = '--', label = 'KDE fit - Training with synthetic data')\n",
    "ax = s2.plot.hist(alpha=0.5, density=10, label = 'Training with observational data')\n",
    "ax = s2.plot.kde(linewidth = 2, secondary_y=True, linestyle = '--', label = 'KDE fit - Training with observational data')\n",
    "\n",
    "plt.legend(fontsize = 18)\n",
    "plt.xlabel('z', fontsize = 24)\n",
    "plt.ylabel('n(z)',fontsize = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram((ymax - ymin)*(ymin + y_test), bins = 50)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(143, figsize = (16, 12))\n",
    "sns.distplot(s, bins = 50)\n",
    "sns.distplot(s1)\n",
    "sns.distplot(s2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmaNMAD(z_spec, z_pho):\n",
    "    return 1.48*np.median( np.abs( z_pho - z_spec)/(1 + z_spec))\n",
    "    # else: return 1.48*np.median( np.abs( z_pho - z_spec)/(1 + z_spec),)\n",
    "\n",
    "def outlierFrac(z_spec, z_pho, threshold = 0.15):\n",
    "    outliers = z_pho[ (np.abs(z_spec - z_pho)) >= threshold*z_spec ]\n",
    "    return 100.0*len(outliers)/np.shape(z_pho)[0]\n",
    "\n",
    "def MedianProperScore(z_spec, mu_all, sigma_all):\n",
    "    ## Eq 27 in https://www.tandfonline.com/doi/pdf/10.1198/016214506000001437?needAccess=true\n",
    "    ## taking median of all scores\n",
    "    S_all = np.zeros_like(mu_all)\n",
    "    for comp_i in range(S_all.shape[1]):\n",
    "        S_all[:, comp_i] = - ((z_spec - mu_all[:, comp_i])/sigma_all[:, comp_i])**2 - np.log(sigma_all[:, comp_i]**2)\n",
    "    return np.median(S_all, axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "bins = np.linspace(0, 1, 20)\n",
    "\n",
    "z_spec = (ymax - ymin)*(ymin + y_test)\n",
    "z_spec2 = (ymax2 - ymin2)*(ymin2 + y_test2)\n",
    "\n",
    "z_phot = (ymax - ymin)*(ymin + y_pred)\n",
    "z_phot2 = (ymax2 - ymin2)*(ymin2 + y_pred_2)\n",
    "\n",
    "z_spec_digitize = np.digitize( z_spec, bins)\n",
    "z_spec_digitize2 = np.digitize( z_spec2, bins)\n",
    "\n",
    "sigmaNMAD_combine = np.zeros(shape=bins.shape[0])\n",
    "outFr_combine = np.zeros(shape=bins.shape[0])\n",
    " \n",
    "\n",
    "sigmaNMAD_obs = np.zeros(shape=bins.shape[0])\n",
    "outFr_obs = np.zeros(shape=bins.shape[0])\n",
    "\n",
    "\n",
    "for ind in range(bins.shape[0] - 1):\n",
    "    z_spec2_bin_z2 =  z_spec2[ z_spec_digitize2  == ind + 1]\n",
    "    z_phot2_bin_z2 =  z_phot2[ z_spec_digitize2  == ind + 1]\n",
    "    sigmaNMAD_obs[ind] =  sigmaNMAD(z_spec2_bin_z2, z_phot2_bin_z2)\n",
    "    outFr_obs[ind] = outlierFrac(z_spec2_bin_z2, z_phot2_bin_z2, 0.15)\n",
    "    \n",
    "    z_spec_bin_z =  z_spec[ np.where(z_spec_digitize  == ind + 1) ]\n",
    "    z_phot_bin_z =  z_phot[ np.where(z_spec_digitize  == ind + 1)]\n",
    "    sigmaNMAD_combine[ind] =  sigmaNMAD(z_spec_bin_z, z_phot_bin_z)\n",
    "    outFr_combine[ind] = outlierFrac(z_spec_bin_z, z_phot_bin_z, 0.15)\n",
    "    \n",
    "\n",
    "plt.figure(5232, figsize=(11,6))\n",
    "\n",
    "bincenter = (bins[1:] + bins[:-1]) / 2.\n",
    "\n",
    "plt.plot(bincenter, sigmaNMAD_obs[:bins.shape[0]-1], 'bo--', label = 'SDSS training')\n",
    "plt.plot(bincenter, sigmaNMAD_combine[:bins.shape[0]-1] , 'ro--', label = 'GALAXPY training')\n",
    "# plt.xscale('log')\n",
    "plt.title(r'$\\sigma_{NMAD}$')\n",
    "leg = plt.legend()\n",
    "\n",
    "for artist, text in zip(leg.legendHandles, leg.get_texts()):\n",
    "    col = artist.get_color()\n",
    "    if isinstance(col, np.ndarray):\n",
    "        col = col[0]\n",
    "    text.set_color(col)\n",
    "    text.set_alpha(1.0)\n",
    "\n",
    "\n",
    "plt.figure(5233, figsize=(11,6))\n",
    "\n",
    "bincenter = (bins[1:] + bins[:-1]) / 2.\n",
    "\n",
    "plt.plot(bincenter, outFr_obs[:bins.shape[0]-1], 'bo--', label = 'SDSS training')\n",
    "plt.plot(bincenter, outFr_combine[:bins.shape[0]-1] , 'ro--', label = 'GALAXPY training')\n",
    "# plt.xscale('log')\n",
    "plt.title(r'$Outlier$ $fraction$')\n",
    "leg = plt.legend()\n",
    "\n",
    "for artist, text in zip(leg.legendHandles, leg.get_texts()):\n",
    "    col = artist.get_color()\n",
    "    if isinstance(col, np.ndarray):\n",
    "        col = col[0]\n",
    "    text.set_color(col)\n",
    "    text.set_alpha(1.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "MedianProperScore_combine_all = MedianProperScore(z_spec, test_means, test_std)\n",
    "MedianProperScore_obs_all = MedianProperScore(z_spec2, test_means_2, test_std_2)\n",
    "\n",
    "print(MedianProperScore_combine_all)\n",
    "print(MedianProperScore_obs_all)\n",
    "\n",
    "\n",
    "\n",
    "MedianProperScore_combine0 = np.zeros(shape=(bins.shape[0], K) )\n",
    "MedianProperScore_obs0 = np.zeros(shape=(bins.shape[0], K) )\n",
    "\n",
    "\n",
    "for ind in range(bins.shape[0] - 1):\n",
    "    z_spec2_bin_z2 =  z_spec2[ z_spec_digitize2  == ind + 1]\n",
    "    test_means_bin_z2 =  test_means_2[ z_spec_digitize2  == ind + 1, :]\n",
    "    test_std_bin_z2 =  test_std_2[ z_spec_digitize2  == ind + 1, :]\n",
    "    MedianProperScore_obs0[ind] =  MedianProperScore(z_spec2_bin_z2, test_means_bin_z2, test_std_bin_z2)\n",
    "    \n",
    "#     z_spec_bin_z =  z_spec[ np.where(z_spec_digitize  == ind + 1) ]\n",
    "#     z_phot_bin_z =  z_phot[ np.where(z_spec_digitize  == ind + 1)]\n",
    "#     sigmaNMAD_combine[ind] =  sigmaNMAD(z_spec_bin_z, z_phot_bin_z)\n",
    "#     outFr_combine[ind] = outlierFrac(z_spec_bin_z, z_phot_bin_z, 0.15)\n",
    "    \n",
    "    z_spec2_bin_z =  z_spec[ z_spec_digitize  == ind + 1]\n",
    "    test_means_bin_z =  test_means[ z_spec_digitize  == ind + 1, :]\n",
    "    test_std_bin_z =  test_std[ z_spec_digitize  == ind + 1, :]\n",
    "    MedianProperScore_combine0[ind] =  MedianProperScore(z_spec2_bin_z, test_means_bin_z, test_std_bin_z)\n",
    "    \n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(3,1, figsize=(11,20), sharex=True)\n",
    "\n",
    "bincenter = (bins[1:] + bins[:-1]) / 2.\n",
    "\n",
    "for score_ind in range(K):\n",
    "    ax[score_ind].plot(bincenter, MedianProperScore_obs0[:bins.shape[0]-1, score_ind], 'bo--', label = 'SDSS training ' + 'Score: ' + str(score_ind))\n",
    "    ax[score_ind].plot(bincenter, MedianProperScore_combine0[:bins.shape[0]-1, score_ind] , 'ro--', label = 'GALAXPY training ' + 'Score: ' + str(score_ind))\n",
    "#     ax[score_ind].set_xscale('log')\n",
    "    ax[score_ind].plot([bincenter[0], bincenter[-1]], [0, 0], 'k-.')\n",
    "    ax[score_ind].set_yscale('symlog')\n",
    "    leg = ax[score_ind].legend()\n",
    "    \n",
    "plt.title(r'$Proper Score $')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightedMedianProperScore(z_spec, mu_all, sigma_all, weights_all):\n",
    "    ## Eq 27 in https://www.tandfonline.com/doi/pdf/10.1198/016214506000001437?needAccess=true\n",
    "    ## taking median of all scores\n",
    "    S_all = np.zeros_like(mu_all)\n",
    "    for comp_i in range(S_all.shape[1]):\n",
    "        S_all[:, comp_i] = - ((z_spec - mu_all[:, comp_i])/sigma_all[:, comp_i])**2 - np.log(sigma_all[:, comp_i]**2)\n",
    "        \n",
    "    WeightedS = weights_all[:, 0]*S_all[:, 0] * weights_all[:, 1]*S_all[:, 1] * weights_all[:, 2]*S_all[:, 2]\n",
    "#     print(WeightedS.shape)\n",
    "    return np.median(WeightedS , axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('obs weighted proper score %.2e'%WeightedMedianProperScore(z_spec2, test_means_2, test_std_2, test_weights_2))\n",
    "print('synthetic weighted proper score %.2e'%WeightedMedianProperScore(z_spec, test_means, test_std, test_weights))\n",
    "\n",
    "\n",
    "\n",
    "WeightedMedianProperScore_combine0 = np.zeros(shape=(bins.shape[0]) )\n",
    "WeightedMedianProperScore_obs0 = np.zeros(shape=(bins.shape[0]) )\n",
    "\n",
    "\n",
    "for ind in range(bins.shape[0] - 1):\n",
    "    z_spec2_bin_z2 =  z_spec2[ z_spec_digitize2  == ind + 1]\n",
    "    test_means_bin_z2 =  test_means_2[ z_spec_digitize2  == ind + 1, :]\n",
    "    test_std_bin_z2 =  test_std_2[ z_spec_digitize2  == ind + 1, :]\n",
    "    test_weights_bin_z2 =  test_weights_2[ z_spec_digitize2  == ind + 1, :]\n",
    "    \n",
    "    WeightedMedianProperScore_obs0[ind] =  WeightedMedianProperScore(z_spec2_bin_z2, test_means_bin_z2, test_std_bin_z2,  test_weights_bin_z2)\n",
    "    \n",
    "#     z_spec_bin_z =  z_spec[ np.where(z_spec_digitize  == ind + 1) ]\n",
    "#     z_phot_bin_z =  z_phot[ np.where(z_spec_digitize  == ind + 1)]\n",
    "#     sigmaNMAD_combine[ind] =  sigmaNMAD(z_spec_bin_z, z_phot_bin_z)\n",
    "#     outFr_combine[ind] = outlierFrac(z_spec_bin_z, z_phot_bin_z, 0.15)\n",
    "    \n",
    "    z_spec2_bin_z =  z_spec[ z_spec_digitize  == ind + 1]\n",
    "    test_means_bin_z =  test_means[ z_spec_digitize  == ind + 1, :]\n",
    "    test_std_bin_z =  test_std[ z_spec_digitize  == ind + 1, :]\n",
    "    test_weights_bin_z =  test_weights[ z_spec_digitize  == ind + 1, :]\n",
    "    \n",
    "    WeightedMedianProperScore_combine0[ind] =  WeightedMedianProperScore(z_spec2_bin_z, test_means_bin_z, test_std_bin_z, test_weights_bin_z)\n",
    "    \n",
    "    \n",
    "    \n",
    "# fig, ax = plt.subplots(1,1, figsize=(11,20), sharex=True)\n",
    "\n",
    "plt.figure(52553, figsize=(11,6))\n",
    "bincenter = (bins[1:] + bins[:-1]) / 2.\n",
    "\n",
    "plt.plot(bincenter, WeightedMedianProperScore_obs0[:bins.shape[0]-1], 'bo--', label = 'SDSS training ' + 'Weighted Score')\n",
    "plt.plot(bincenter, WeightedMedianProperScore_combine0[:bins.shape[0]-1] , 'ro--', label = 'GALAXPY training ' + 'Weighted Score')\n",
    "# ax[score_ind].set_xscale('log')\n",
    "plt.plot([bincenter[0], bincenter[-1]], [0, 0], 'k-.')\n",
    "plt.yscale('symlog')\n",
    "leg = plt.legend()\n",
    "    \n",
    "plt.title(r'$Weighted Proper Score $')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedDiff(z_spec, z_pho):\n",
    "    return ( z_pho - z_spec)/(1 + z_spec)\n",
    "\n",
    "# fig.add_subplot(212, adjustable='box', aspect=0.3)\n",
    "# plt.plot( (ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), 'ro')\n",
    "# plt.plot( (ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), 'bo')\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "bins = np.linspace(0, 1, 0)\n",
    "#=======\n",
    "# z_spec_bin = np.histogram( (ymax2 - ymin2)*(ymin2 + y_test2), bins)[0]\n",
    "# z_phot_bin = np.histogram( (ymax2 - ymin2)*(ymin2 + y_pred_new2), bins)[0]\n",
    "\n",
    "\n",
    "z_spec2 = (ymax2 - ymin2)*(ymin2 + y_test2)\n",
    "z_phot2 = (ymax2 - ymin2)*(ymin2 + y_pred_2)\n",
    "\n",
    "z_spec_digitize2 = np.digitize( z_spec2, bins)\n",
    "\n",
    "# for n in range(z_spec.size):\n",
    "#     print(bins[z_spec_digitize[n]-1], \"<=\", z_spec[n], \"<\", bins[z_spec_digitize[n]])\n",
    "\n",
    "\n",
    "sigmaNMAD_combine = np.zeros(shape=bins.shape[0]-1)\n",
    "outFr_combine = np.zeros(shape=bins.shape[0]-1)\n",
    "\n",
    "\n",
    "for ind in range(bins.shape[0] - 1):\n",
    "    z_spec2_bin_z2 =  z_spec2[ z_spec_digitize2  == ind + 1]\n",
    "    z_phot2_bin_z2 =  z_phot2[ z_spec_digitize2  == ind + 1]\n",
    "    sigmaNMAD_combine[ind] =  sigmaNMAD(z_spec2_bin_z2, z_phot2_bin_z2)\n",
    "    outFr_combine[ind] = outlierFrac(z_spec2_bin_z2, z_phot2_bin_z2, 0.15)\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "z_spec = (ymax - ymin)*(ymin + y_test)\n",
    "z_phot = (ymax - ymin)*(ymin + y_pred)\n",
    "\n",
    "z_spec_digitize = np.digitize( z_spec, bins)\n",
    "\n",
    "# for n in range(z_spec.size):\n",
    "#     print(bins[z_spec_digitize[n]-1], \"<=\", z_spec[n], \"<\", bins[z_spec_digitize[n]])\n",
    "\n",
    "\n",
    "sigmaNMAD_obs = np.zeros(shape=binshape)\n",
    "outFr_obs = np.zeros(shape=binshape)\n",
    "\n",
    "\n",
    "for ind in range(bins.shape[0]- 1):\n",
    "    z_spec_bin_z =  z_spec2[ np.where(z_spec_digitize  == ind + 1) ]\n",
    "    z_phot_bin_z =  z_phot2[ np.where(z_spec_digitize  == ind + 1)]\n",
    "\n",
    "    sigmaNMAD_obs[ind] =  sigmaNMAD(z_spec_bin_z, z_phot_bin_z)\n",
    "    outFr_obs[ind] = outlierFrac(z_spec_bin_z, z_phot_bin_z, 0.15)\n",
    "\n",
    "\n",
    "plt.figure(5232)\n",
    "\n",
    "bincenter = (bins[1:] + bins[:-1]) / 2.\n",
    "\n",
    "plt.plot(bincenter, sigmaNMAD_obs[:bins.shape[0]-1], 'ro--', label = 'SDSS training')\n",
    "plt.plot(bincenter, sigmaNMAD_combine[:bins.shape[0]-1] , 'bo--', label = 'GalaxPy training')\n",
    "plt.xscale('log')\n",
    "plt.title(r'$\\sigma_{NMAD}$')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure(5233)\n",
    "\n",
    "bincenter = (bins[1:] + bins[:-1]) / 2.\n",
    "\n",
    "plt.plot(bincenter, outFr_obs[:bins.shape[0]-1], 'ro--', label = 'SDSS training')\n",
    "plt.plot(bincenter, outFr_combine[:bins.shape[0]-1] , 'bo--', label = 'GalaxPy training')\n",
    "plt.xscale('log')\n",
    "plt.title('outlier fraction')\n",
    "plt.legend()\n",
    "\n",
    "sigmaNMAD_obs = np.zeros_like(bins)\n",
    "\n",
    "for ind in range(bins.shape[0]):\n",
    "    z_spec_bin_z =  z_spec2[ np.where(z_spec_digitize  == ind + 1) ]\n",
    "    z_phot_bin_z =  z_phot2[ np.where(z_spec_digitize  == ind + 1)]\n",
    "    sigmaNMAD_obs[ind] =  sigmaNMAD(z_spec_bin_z, z_phot_bin_z)\n",
    "\n",
    "\n",
    "plt.figure(5232, figsize=(8,6))\n",
    "\n",
    "bincenter = (bins[1:] + bins[:-1]) / 2.\n",
    "\n",
    "plt.plot(bincenter, sigmaNMAD_obs[:-1], 'rx--', label = 'SDSS training')\n",
    "plt.plot(bincenter, sigmaNMAD_combine[:-1] , 'bx--', label = 'GalaxPy training')\n",
    "# plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.ylabel(r'$\\sigma_{NMAD}(z)$', fontsize=19)\n",
    "plt.xlabel(r'$z_{spec}$', fontsize=19)\n",
    "\n",
    "# plt.savefig('sNMAD_compare.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "# sigmaNMAD(z_spec_bin, z_phot_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 2\n",
    "z_phot2[ np.where(z_spec_digitize  == ind + 1)]\n",
    "z_phot[ np.where(z_spec_digitize  == ind + 1)]\n",
    "\n",
    "z_phot == z_phot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifTrialPlots = False\n",
    "\n",
    "\n",
    "if ifTrialPlots:\n",
    "    plt.figure(22, figsize=(7, 7))\n",
    "\n",
    "    y_pred = np.sum(pred_means_new*pred_weights_new, axis = 1)\n",
    "    y_pred_std_new = np.sum(pred_std_new*pred_weights_new, axis = 1)\n",
    "\n",
    "    # # plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "    # plt.errorbar((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(\n",
    "    #   ymin + y_pred_std_new), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "    #\n",
    "    plt.errorbar((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(\n",
    "      ymin + y_pred_std_new), fmt='ro', ecolor='r', ms = 3, alpha = 0.3, label = 'SDSS trained '\n",
    "                                                                                 'model')\n",
    "\n",
    "\n",
    "\n",
    "    # plt.text(0.8, 2.0, datafile, horizontalalignment='center', verticalalignment='center')\n",
    "    # plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "    # plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "\n",
    "    # plt.title('weight x mean')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "## Overall mean --- weight * mean\n",
    "\n",
    "if ifTrialPlots:\n",
    "\n",
    "    y_pred_new2 = np.sum(pred_means_new2*pred_weights_new2, axis = 1)\n",
    "    y_pred_std_new2 = np.sum(pred_std_new2*pred_weights_new2, axis = 1)\n",
    "\n",
    "    plt.figure(22, figsize=(6,6))\n",
    "\n",
    "\n",
    "    # # plt.scatter(y_test2, y_pred, facecolors='k', s = 1)\n",
    "    # plt.errorbar((ymax2 - ymin2)*(ymin2 + y_test2), (ymax2 - ymin2)*(ymin2 + y_pred), yerr= (ymax2 - ymin2)*(\n",
    "    #   ymin2 + y_pred_std_new2), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "    #\n",
    "    plt.errorbar((ymax2 - ymin2)*(ymin2 + y_test2), (ymax2 - ymin2)*(ymin2 + y_pred_new2), yerr= (ymax2 - ymin2)*(\n",
    "      ymin2 + y_pred_std_new2), fmt='bo', ecolor='b', ms = 3, alpha = 0.3, label = 'GALAXPY '\n",
    "                                                                                   'trained model')\n",
    "\n",
    "\n",
    "fig = plt.figure(22)\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# plt.text(0.1, 0.9, r'$\\sigma_{NMAD}$ = %.3f'%sigmaNMAD_obs_all, color = 'red' , size = 20)\n",
    "# plt.text(0.1, 0.85, r'$\\sigma_{NMAD}$ = %.3f'%sigmaNMAD_combine_all, color = 'blue' , size = 20)\n",
    "plt.text(0.6, 0.2, r'$\\sigma_{NMAD}$ = %.3f'%sigmaNMAD_obs_all, color = 'red' , size = 15)\n",
    "plt.text(0.6, 0.1, r'$\\sigma_{NMAD}$ = %.3f'%sigmaNMAD_combine_all, color = 'blue' , size = 15)\n",
    "\n",
    "# plt.text(0.1, 0.9, r'GalaxPy training', color = 'red' , size = 20)\n",
    "# plt.text(0.1, 0.85, r'SDSS training', color = 'blue' , size = 20)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k')\n",
    "plt.plot([0, 1], 0.85*np.array([0, 1]), 'k-.')\n",
    "plt.plot([0, 1], 1.15*np.array([0, 1]), 'k-.')\n",
    "\n",
    "plt.ylabel(r'$z_{phot}$', fontsize=19)\n",
    "plt.xlabel(r'$z_{spec}$', fontsize=19)\n",
    "\n",
    "\n",
    "# plt.ylabel(r'Photometric redshift', fontsize=19)\n",
    "# plt.xlabel(r'Spectroscopic redshift', fontsize=19)\n",
    "plt.xlim(0.0, 1)\n",
    "plt.ylim(0.0, 1)\n",
    "\n",
    "# plt.legend(fontsize = 'large', markerscale=3., numpoints=3)\n",
    "# plt.title('weight x mean')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "\n",
    "leg = plt.legend(fontsize = 'large', markerscale=2., numpoints=2)\n",
    "\n",
    "for artist, text in zip(leg.legendHandles, leg.get_texts()):\n",
    "    col = artist.get_color()\n",
    "    if isinstance(col, np.ndarray):\n",
    "        col = col[0]\n",
    "    text.set_color(col)\n",
    "    text.set_alpha(1.0)\n",
    " \n",
    "\n",
    "plt.savefig('phoz_compare.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:env_py37] *",
   "language": "python",
   "name": "conda-env-env_py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
