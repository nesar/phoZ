{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n",
      "=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~=~\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "from datetime import datetime\n",
    "import os\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt;\n",
    "import random\n",
    "\n",
    "\n",
    "import matplotlib.axes as axes;\n",
    "from matplotlib.patches import Ellipse\n",
    "import seaborn as sns;\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "print(20*'=~')\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "# writer = tf.summary.FileWriter('./log_dir', sess.graph)\n",
    "# tf.contrib.summary.create_file_writer('./log_dir', sess.graph)\n",
    "print(20*'=~')\n",
    "\n",
    "# https://github.com/dunovank/jupyter-themes\n",
    "# jt -t onedork -fs 14 -altp -tfs 14 -nfs 14 -ofs 14 -cellw 90% -T -N -kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReadGalaxPy(path_program = '../../Data/fromGalaxev/photozs/datasets/', sim_obs_combine = True):\n",
    "    import os\n",
    "    import sys\n",
    "    import glob\n",
    "    from astropy.table import Table\n",
    "\n",
    "    # path_program = '../../Data/fromGalaxev/photozs/datasets/'\n",
    "\n",
    "\n",
    "    class Curated_sample():\n",
    "        ''' Class to store the redshift and colors of observed galaxies,\n",
    "            and the redshift, Mpeak, colors, and \"weights\" of simulated\n",
    "            galaxies whose colors are compatible with those of observed\n",
    "            galaxies.\n",
    "\n",
    "            The observed sample include galaxies from SDSS\n",
    "            (SDSS+BOSS+eBOSS), DEEP2, and VIPERS.\n",
    "\n",
    "            The simulated sample was created by sampling the parameter of\n",
    "            GALAXPY using a LH.\n",
    "\n",
    "            The weights of simulated galaxies are related to the number\n",
    "            density of observed galaxies in the same region of the color\n",
    "            space.\n",
    "\n",
    "            You only have to care about the method load_structure. '''\n",
    "\n",
    "        def __init__(self):\n",
    "            self.arr_c = []\n",
    "            self.arr_z = []\n",
    "            self.arr_m = []\n",
    "            self.arr_w = []\n",
    "\n",
    "        def append(self, c, z, m, w):\n",
    "            self.arr_c.append(c)\n",
    "            self.arr_z.append(z)\n",
    "            self.arr_m.append(m)\n",
    "            self.arr_w.append(w)\n",
    "\n",
    "        def ndarray(self):\n",
    "            self.arr_c = np.concatenate(self.arr_c)\n",
    "            self.arr_z = np.concatenate(self.arr_z)\n",
    "            self.arr_m = np.concatenate(self.arr_m)\n",
    "            self.arr_w = np.concatenate(self.arr_w)\n",
    "\n",
    "        def save_struct(self, name):\n",
    "            np.save(name + 'c.npy', self.arr_c)\n",
    "            np.save(name + 'z.npy', self.arr_z)\n",
    "            np.save(name + 'm.npy', self.arr_m)\n",
    "            np.save(name + 'w.npy', self.arr_w)\n",
    "\n",
    "        def load_struct(self, name):\n",
    "            self.arr_c = np.load(name + 'c.npy')\n",
    "            self.arr_z = np.load(name + 'z.npy')\n",
    "            self.arr_m = np.load(name + 'm.npy')\n",
    "            self.arr_w = np.load(name + 'w.npy')\n",
    "\n",
    "        def duplicate_data(self, zrange):\n",
    "            aa = np.where((self.arr_w > 50)\n",
    "                          & (self.arr_z >= zrange[0])\n",
    "                          & (self.arr_z < zrange[1]))[0]\n",
    "            print(aa.shape)\n",
    "            cc = np.repeat(aa, self.arr_w[aa].astype(int))\n",
    "            self.arr_cn = self.arr_c[cc, :]\n",
    "            self.arr_zn = self.arr_z[cc]\n",
    "            self.arr_mn = self.arr_m[cc]\n",
    "\n",
    "\n",
    "    def read_curated_data():\n",
    "        run_path = path_program + 'runs/run_z3/'\n",
    "\n",
    "        sim_q = Curated_sample()  # simulated colors quenched galaxies\n",
    "        sim_s = Curated_sample()  # simulated colors star-forming galaxies\n",
    "        obs_q = Curated_sample()  # observed colors quenched galaxies\n",
    "        obs_s = Curated_sample()  # observed colors star-forming galaxies\n",
    "\n",
    "        obs_q.load_struct(run_path + 'str_obs_q')\n",
    "        obs_s.load_struct(run_path + 'str_obs_s')\n",
    "        sim_q.load_struct(run_path + 'str_sim_q')\n",
    "        sim_s.load_struct(run_path + 'str_sim_s')\n",
    "\n",
    "        print(sim_q.arr_c.shape)\n",
    "        print(sim_s.arr_c.shape)\n",
    "        print(obs_q.arr_c.shape)\n",
    "        print(obs_s.arr_c.shape)\n",
    "\n",
    "        return sim_q, sim_s, obs_q, obs_s\n",
    "\n",
    "\n",
    "    sim_q, sim_s, obs_q, obs_s = read_curated_data()\n",
    "\n",
    "    if sim_obs_combine:\n",
    "        train_datafile = 'GalaxPy'\n",
    "\n",
    "        # 2.0 ####### TRAIN USING SIMULATION, TEST OBSERVATION ####\n",
    "\n",
    "        Trainfiles = np.append(sim_q.arr_c, sim_s.arr_c, axis=0)\n",
    "        TrainZ = np.append(sim_q.arr_z, sim_s.arr_z, axis=0)\n",
    "\n",
    "        Trainfiles = np.delete(Trainfiles, (4), axis=1)  ## deleting z-Y\n",
    "\n",
    "        Testfiles = np.append(obs_q.arr_c, obs_s.arr_c, axis=0)\n",
    "        TestZ = np.append(obs_q.arr_z, obs_s.arr_z, axis=0)\n",
    "\n",
    "        TrainshuffleOrder = np.arange(Trainfiles.shape[0])\n",
    "        np.random.shuffle(TrainshuffleOrder)\n",
    "\n",
    "        Trainfiles = Trainfiles[TrainshuffleOrder]\n",
    "        TrainZ = TrainZ[TrainshuffleOrder]\n",
    "\n",
    "        TestshuffleOrder = np.arange(Testfiles.shape[0])\n",
    "        np.random.shuffle(TestshuffleOrder)\n",
    "\n",
    "        Testfiles = Testfiles[TestshuffleOrder]\n",
    "        TestZ = TestZ[TestshuffleOrder]\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[:num_test]  # color mag\n",
    "\n",
    "        y_train = TrainZ[:num_train]  # spec z\n",
    "        y_test = TrainZ[:num_test]  # spec z\n",
    "\n",
    "    else:\n",
    "        train_datafile = 'SDSS'\n",
    "        # 1.1 ####### SIMULATED: QUENCHED ONLY ############\n",
    "        # Trainfiles = sim_q.arr_c\n",
    "        # TrainZ = sim_q.arr_z\n",
    "\n",
    "        # 1.2 ### SIMULATED: QUENCHED + STAR FORMATION ####\n",
    "\n",
    "        # Trainfiles =np.append( sim_q.arr_c, sim_s.arr_c, axis = 0)\n",
    "        # TrainZ = np.append( sim_q.arr_z, sim_s.arr_z, axis = 0)\n",
    "\n",
    "        # 1.3 ####### OBSERVED: QUENCHED + STAR FORMATION ####\n",
    "\n",
    "        Trainfiles = np.append(obs_q.arr_c, obs_s.arr_c, axis=0)\n",
    "        TrainZ = np.append(obs_q.arr_z, obs_s.arr_z, axis=0)\n",
    "\n",
    "        TrainshuffleOrder = np.arange(Trainfiles.shape[0])\n",
    "        np.random.shuffle(TrainshuffleOrder)\n",
    "\n",
    "        Trainfiles = Trainfiles[TrainshuffleOrder]\n",
    "        TrainZ = TrainZ[TrainshuffleOrder]\n",
    "\n",
    "        # 1 #################################\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[num_train + 1: num_train + num_test]  # color mag\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[num_train + 1: num_train + num_test]  # color mag\n",
    "\n",
    "        y_train = TrainZ[:num_train]  # spec z\n",
    "        y_test = TrainZ[num_train + 1: num_train + num_test]  # spec z\n",
    "\n",
    "    ############## THINGS ARE SAME AFTER THIS ###########\n",
    "\n",
    "    ## rescaling xmax/xmin\n",
    "    xmax = np.max([np.max(X_train, axis=0), np.max(X_test, axis=0)], axis=0)\n",
    "    xmin = np.min([np.min(X_train, axis=0), np.min(X_test, axis=0)], axis=0)\n",
    "\n",
    "    X_train = (X_train - xmin) / (xmax - xmin)\n",
    "    X_test = (X_test - xmin) / (xmax - xmin)\n",
    "\n",
    "    #### RESCALING X_train, X_test NOT done yet -- (g-i), (r-i) ... and i mag -->> Color/Mag issue\n",
    "\n",
    "    ymax = np.max([y_train.max(), y_test.max()])\n",
    "    ymin = np.min([y_train.min(), y_test.min()])\n",
    "\n",
    "    y_train = (y_train - ymin) / (ymax - ymin)\n",
    "    y_test = (y_test - ymin) / (ymax - ymin)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, ymax, ymin, xmax, xmin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(tensors):\n",
    "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
    "    Args:\n",
    "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
    "      `namedtuple` or combinations thereof.\n",
    "\n",
    "    Returns:\n",
    "      ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
    "        `EagerTensor`s replaced by Numpy `ndarray`s.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        return tf.contrib.framework.nest.pack_sequence_as(\n",
    "            tensors,\n",
    "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    return sess.run(tensors)\n",
    "\n",
    "def plot_normal_mix(pis, mus, sigmas, ax, label='', comp=True):\n",
    "  \"\"\"Plots the mixture of Normal models to axis=ax comp=True plots all\n",
    "  components of mixture model\n",
    "  \"\"\"\n",
    "  # x = np.linspace(-10.5, 10.5, 250)\n",
    "  x = np.linspace(-0.1, 1.1, 250)\n",
    "  final = np.zeros_like(x)\n",
    "  for i, (weight_mix, mu_mix, sigma_mix) in enumerate(zip(pis, mus, sigmas)):\n",
    "    temp = stats.norm.pdf(x, mu_mix, sigma_mix) * weight_mix\n",
    "    final = final + temp\n",
    "    if comp:\n",
    "      ax.plot(x, temp, label='Normal ' + str(i))\n",
    "  ax.plot(x, final, label='Mixture of Normals ' + label)\n",
    "  ax.legend(fontsize=13)\n",
    "  return final\n",
    "\n",
    "\n",
    "\n",
    "def neural_network_mod():\n",
    "    \"\"\"\n",
    "    loc, scale, logits = NN(x; theta)\n",
    "\n",
    "    Args:\n",
    "      X: Input Tensor containing input data for the MDN\n",
    "    Returns:\n",
    "      locs: The means of the normal distributions that our data is divided into.\n",
    "      scales: The scales of the normal distributions that our data is divided\n",
    "        into.\n",
    "      logits: The probabilities of ou categorical distribution that decides\n",
    "        which normal distribution our data points most probably belong to.\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float64,name='X',shape=(None,D))\n",
    "    # 2 hidden layers with 15 hidden units\n",
    "    net = tf.layers.dense(X, 32, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 16, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 8, activation=tf.nn.relu)\n",
    "    locs = tf.layers.dense(net, K, activation=None)\n",
    "    scales = tf.layers.dense(net, K, activation=tf.exp)\n",
    "    logits = tf.layers.dense(net, K, activation=None)\n",
    "    outdict= {'locs':locs, 'scales':scales, 'logits':logits}\n",
    "    hub.add_signature(inputs=X,outputs=outdict)\n",
    "\n",
    "    return locs, scales, logits\n",
    "\n",
    "\n",
    "def mixture_model(X,Y,learning_rate=1e-3,decay_rate=.95,step=1000,train=True):\n",
    "    if train:\n",
    "        dict = neural_network(tf.convert_to_tensor(X),as_dict=True)\n",
    "    else:\n",
    "        dict = neural_network_t(tf.convert_to_tensor(X),as_dict=True)\n",
    "    locs = dict['locs'] ; scales = dict['scales'] ; logits = dict['logits']\n",
    "    cat = tfd.Categorical(logits=logits)\n",
    "    components = [tfd.Normal(loc=loc, scale=scale) for loc, scale\n",
    "                  in zip(tf.unstack(tf.transpose(locs)),\n",
    "                         tf.unstack(tf.transpose(scales)))]\n",
    "\n",
    "    y = tfd.Mixture(cat=cat, components=components)\n",
    "    #define loss function\n",
    "    log_likelihood = y.log_prob(Y)\n",
    "    # log_likelihood = -tf.reduce_sum(log_likelihood/(1. + y_train)**2 )\n",
    "    y_mean = np.median(Y)\n",
    "    log_likelihood = -tf.reduce_sum(log_likelihood)\n",
    "    #log_likelihood = -tf.reduce_sum(log_likelihood*(y_mean-y_train)**4 )\n",
    "    if train:\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        decayed_lr = tf.train.exponential_decay(learning_rate,\n",
    "                                        global_step, step,\n",
    "                                        decay_rate, staircase=True)\n",
    "        optimizer = tf.train.AdamOptimizer(decayed_lr)\n",
    "        train_op = optimizer.minimize(log_likelihood)\n",
    "        evaluate(tf.global_variables_initializer())\n",
    "        return log_likelihood, train_op, logits, locs, scales\n",
    "    else:\n",
    "        evaluate(tf.global_variables_initializer())\n",
    "        return log_likelihood, logits, locs, scales\n",
    "\n",
    "def train(log_likelihood,train_op,n_epoch):\n",
    "    train_loss = np.zeros(n_epoch)\n",
    "    test_loss = np.zeros(n_epoch)\n",
    "    for i in range(n_epoch):\n",
    "        _, loss_value = evaluate([train_op, log_likelihood])\n",
    "        # summary, loss_value = evaluate([train_op, log_likelihood])\n",
    "\n",
    "        train_loss[i] = loss_value\n",
    "        # writer.add_summary(summary, i)\n",
    "    plt.plot(np.arange(n_epoch), -train_loss / len(X_train), label='Train Loss')\n",
    "    # plt.savefig('../Plots/T_loss_function.pdf')\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def get_predictions(logits,locs,scales):\n",
    "    pred_weights, pred_means, pred_std = evaluate([tf.nn.softmax(logits), locs, scales])\n",
    "    return pred_weights, pred_means, pred_std\n",
    "\n",
    "def plot_pdfs(pred_means,pred_weights,pred_std,num=6,train=True):\n",
    "    if train:\n",
    "        obj = [random.randint(0,num_train-1) for x in range(num)]\n",
    "    else:\n",
    "        obj = [random.randint(0,num_test-1) for x in range(num)]\n",
    "    #obj = [93, 402, 120,789,231,4,985]\n",
    "    print(obj)\n",
    "    fig, axes = plt.subplots(nrows=num, ncols=1, sharex = True, figsize=(8, 7))\n",
    "    allfs = []\n",
    "    for i in range(len(obj)):\n",
    "        fs = plot_normal_mix(pred_weights[obj][i], pred_means[obj][i],\n",
    "                    pred_std[obj][i], axes[i], comp=False)\n",
    "        allfs.append(fs)\n",
    "        axes[i].axvline(x=y_train[obj][i], color='black', alpha=0.5)\n",
    "        axes[i].text(0.3, 4.0, 'ID: ' +str(obj[i]), horizontalalignment='center',\n",
    "        verticalalignment='center')\n",
    "\n",
    "    plt.xlabel(r' rescaled[$z_{pred}]$', fontsize = 19)\n",
    "    # plt.savefig('../Plots/T_pdfs.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def plot_pred_mean(pred_means,pred_weights,pred_std,ymax,ymin,y_train,select='no'):\n",
    "    y_pred = np.sum(pred_means*pred_weights, axis = 1)\n",
    "    y_pred_std = np.sum(pred_std*pred_weights, axis = 1)\n",
    "\n",
    "    plt.figure(22, figsize=(9,8))\n",
    "\n",
    "    #ymax=1\n",
    "    #ymin=0\n",
    "    # if select == 'yes':\n",
    "    #     y_pred = y_pred[obj]\n",
    "    #     y_train = y_train[obj]\n",
    "    #     y_pred_std = y_pred_std[obj]\n",
    "\n",
    "    # plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "\n",
    "    plt.errorbar( (ymax - ymin)*(y_train)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "    #switched\n",
    "    #plt.errorbar(  (ymax - ymin)*(y_pred)+ymin, (ymax - ymin)*(y_train)+ymin, yerr= (ymax - ymin)*(y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "    #plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "    plt.plot((ymax - ymin)*(y_train)+ymin, (ymax - ymin)*( y_train)+ymin, 'k')\n",
    "\n",
    "    plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "    plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "    #plt.xlim([0,1])\n",
    "    #plt.ylim([0,1])\n",
    "    plt.title('weight x mean')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('../Plots/T_pred_mean.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def plot_pred_peak(pred_means,pred_weights,pred_std,ymax,ymin,y_train,select='no'):\n",
    "    def peak(weight,sigma):\n",
    "        return weight/np.sqrt(2*np.pi*sigma**2)\n",
    "\n",
    "    peak_max = np.argmax(peak(pred_weights,pred_std),axis=1)\n",
    "    y_pred = np.array([pred_means[i,peak_max[i]] for i in range(len(y_train))])\n",
    "    y_pred_std = np.array([pred_std[i,peak_max[i]] for i in range(len(y_train))])\n",
    "    plt.figure(24, figsize=(9, 8))\n",
    "    # if select == 'yes':\n",
    "    #     y_pred = y_pred[obj]\n",
    "    #     y_train = y_train[obj]\n",
    "    #     y_pred_std = y_pred_std[obj]\n",
    "    # plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "    plt.errorbar((ymax - ymin)*(y_train)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(\n",
    "      y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "    #plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "    plt.plot((ymax - ymin)*(y_test)+ymin, (ymax - ymin)*(y_test)+ymin, 'k')\n",
    "    plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "    plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "    #plt.xlim([0,1])\n",
    "    #plt.ylim([0,1])\n",
    "    plt.title('highest peak')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pred_weight(pred_means,pred_weights,pred_std,ymax,ymin,y_train,select='no'):\n",
    "    weight_max = np.argmax(pred_weights, axis = 1)  ## argmax or max???\n",
    "\n",
    "    y_pred = np.array([pred_means[i,weight_max[i]] for i in range(len(y_train))])\n",
    "    y_pred_std = np.array([pred_std[i,weight_max[i]] for i in range(len(y_train))])\n",
    "\n",
    "    plt.figure(29, figsize=(9, 8))\n",
    "    # if select == 'yes':\n",
    "    #     y_pred = y_pred[obj]\n",
    "    #     y_train = y_train[obj]\n",
    "    #     y_pred_std = y_pred_std[obj]\n",
    "\n",
    "    # plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "    plt.errorbar((ymax - ymin)*(y_train)+ymin, (ymax - ymin)*(y_pred)+ymin, yerr= (ymax - ymin)*(\n",
    "      y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "    #plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "    plt.plot((ymax - ymin)*(y_test)+ymin, (ymax - ymin)*(y_test)+ymin, 'k')\n",
    "    plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "    plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "    #plt.xlim([0,1])\n",
    "    #plt.ylim([0,1])\n",
    "    plt.title('highest weight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def per_stats(pred_means,pred_weights,pred_std,ymax,ymin,y_train):\n",
    "    y_pred = np.sum(pred_means*pred_weights, axis = 1)\n",
    "    y_pred_std = np.sum(pred_std*pred_weights, axis = 1)\n",
    "    y_pred = (ymax - ymin)*(y_pred)+ymin\n",
    "    y_pred_std = (ymax - ymin)*(y_pred_std)\n",
    "    y_train = (ymax - ymin)*(y_train)+ymin\n",
    "    diff = y_pred-y_train\n",
    "    mean_diff = np.mean(diff)\n",
    "    med_diff = np.median(diff)\n",
    "    std_diff = np.std(diff)\n",
    "    mean_sigma = np.mean(y_pred_std)\n",
    "    med_sigma = np.median(y_pred_std)\n",
    "    std_sigma = np.std(y_pred_std)\n",
    "    return mean_diff, med_diff, std_diff, mean_sigma, med_sigma, std_sigma\n",
    "\n",
    "\n",
    "def testing(X_test,y_test):\n",
    "\n",
    "    log_likelihood,  logits, locs, scales = mixture_model(X_test,y_test,train=False)\n",
    "    #_, loss_value = evaluate([train_op, log_likelihood])\n",
    "    pred_weights, pred_means, pred_std = get_predictions(logits,locs,scales)\n",
    "    return pred_weights, pred_means, pred_std\n",
    "\n",
    "def plot_cum_sigma(pred_weights,pred_std,ymax,ymin):\n",
    "    #y_pred_std = np.sum(pred_std*pred_weights, axis = 1)\n",
    "\n",
    "    weight_max = np.argmax(pred_weights, axis = 1)  ## argmax or max???\n",
    "    y_pred_std = np.array([pred_std[i,weight_max[i]] for i in range(len(pred_weights[0]))])\n",
    "    y_pred_std = (ymax - ymin)*(y_pred_std)\n",
    "    plt.figure(222)\n",
    "    plt.hist(y_pred_std,100, density=True, histtype='step',\n",
    "                           cumulative=True,color='k')\n",
    "    plt.xlabel('Sigma')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_epochs = 20000 #100000 #1000 #20000 #20000\n",
    "# N = 4000  # number of data points  -- replaced by num_trai\n",
    "D = 5 #6  # number of features  (8 for DES, 6 for COSMOS)\n",
    "K = 3 # number of mixture components\n",
    "\n",
    "\n",
    "learning_rate = 1e-4 #5e-3\n",
    "decay_rate= 0.01 #0.0\n",
    "step=100\n",
    "\n",
    "\n",
    "num_train = 800000 #12000000 #800000\n",
    "num_test = 5000 #params.num_test # 32\n",
    "\n",
    "\n",
    "syntheticTrain = True # True # (sim_obs_combine) True -- train using GalaxyPy, False -- train using\n",
    "\n",
    "save_mod = 'hub_mod_Synthetic_'+str(syntheticTrain)+'_lr_'+str(learning_rate)+'_dr'+str(decay_rate)+'_step'+str(step)+'_ne'+str(n_epochs)+'_k'+str(K)+'_nt'+str(num_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6304534, 6)\n",
      "(7237871, 6)\n",
      "(456685, 5)\n",
      "(423353, 5)\n",
      "Size of features in training data: (800000, 5)\n",
      "Size of output in training data: (800000,)\n",
      "Size of features in test data: (5000, 5)\n",
      "Size of output in test data: (5000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############training\n",
    "\n",
    "# X_train, y_train, X_test, y_test, ymax, ymin, xmax, xmin = ReadGalaxPy(path_program = '../../Data/fromGalaxev/photozs/datasets/', sim_obs_combine = syntheticTrain)\n",
    "X_train, y_train, X_test, y_test, ymax, ymin, xmax, xmin = ReadGalaxPy(path_program = '../../Data/fromGalaxev/photozs/datasets/', sim_obs_combine = syntheticTrain)\n",
    "\n",
    "\n",
    "print(\"Size of features in training data: {}\".format(X_train.shape))\n",
    "print(\"Size of output in training data: {}\".format(y_train.shape))\n",
    "print(\"Size of features in test data: {}\".format(X_test.shape))\n",
    "print(\"Size of output in test data: {}\".format(y_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "net_spec = hub.create_module_spec(neural_network_mod)\n",
    "neural_network = hub.Module(net_spec,name='neural_network',trainable=True)\n",
    "\n",
    "log_likelihood, train_op, logits, locs, scales  = mixture_model(X_train,y_train,learning_rate=learning_rate,decay_rate=decay_rate)\n",
    "\n",
    "train_loss = train(log_likelihood,train_op,n_epochs)\n",
    "#save network\n",
    "neural_network.export(save_mod,sess)\n",
    "\n",
    "pred_weights, pred_means, pred_std = get_predictions(logits, locs, scales)\n",
    "print(pred_means)\n",
    "\n",
    "plot_pdfs(pred_means,pred_weights,pred_std)\n",
    "\n",
    "plot_pred_mean(pred_means,pred_weights,pred_std,ymax,ymin,y_train)\n",
    "\n",
    "mean_diff, med_diff, std_diff, mean_sigma, med_sigma, std_sigma = per_stats(pred_means,pred_weights,pred_std,ymax,ymin,y_train)\n",
    "\n",
    "plot_cum_sigma(pred_weights,pred_std,ymax,ymin)\n",
    "\n",
    "\n",
    "plot_pred_peak(pred_means,pred_weights,pred_std,ymax,ymin,y_train)\n",
    "plot_pred_weight(pred_means,pred_weights,pred_std,ymax,ymin,y_train)\n",
    "\n",
    "#load network\n",
    "neural_network_t = hub.Module(save_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing\n",
    "\n",
    "\n",
    "test_weights, test_means, test_std = testing(X_test,y_test)\n",
    "plot_pdfs(test_means,test_weights,test_std,train=False)\n",
    "\n",
    "plot_pred_mean(test_means,test_weights,test_std,ymax,ymin,y_test)\n",
    "\n",
    "plot_cum_sigma(test_weights,test_std,ymax,ymin)\n",
    "\n",
    "test_mean_diff, test_med_diff, test_std_diff, test_mean_sigma, test_med_sigma, test_std_sigma = per_stats(test_means,test_weights,test_std,ymax,ymin,y_test)\n",
    "\n",
    "plot_pred_peak(test_means,test_weights,test_std,ymax,ymin,y_test)\n",
    "plot_pred_weight(test_means,test_weights,test_std,ymax,ymin,y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_tfp]",
   "language": "python",
   "name": "conda-env-tf_tfp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
