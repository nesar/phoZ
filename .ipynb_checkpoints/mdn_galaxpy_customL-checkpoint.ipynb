{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports and Global Variables  { display-mode: \"form\" }\n",
    "# !pip3 install -q observations\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# @markdown This sets the warning status (default is `ignore`, since this notebook runs correctly)\n",
    "warning_status = \"ignore\"  # @param [\"ignore\", \"always\", \"module\", \"once\", \"default\", \"error\"]\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(warning_status)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(warning_status, category=DeprecationWarning)\n",
    "    warnings.filterwarnings(warning_status, category=UserWarning)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# @markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/))\n",
    "# matplotlib_style = 'default'  # @param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook']\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "# plt.style.use(matplotlib_style)\n",
    "import matplotlib.axes as axes;\n",
    "from matplotlib.patches import Ellipse\n",
    "# @markdown This sets the resolution of the plot outputs (`retina` is the highest resolution)\n",
    "import seaborn as sns;\n",
    "\n",
    "# sns.set_context('notebook')\n",
    "# from IPython.core.pylabtools import figsize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tfe = tf.contrib.eager\n",
    "\n",
    "# Eager Execution\n",
    "# @markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager)\n",
    "# @markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html)\n",
    "use_tf_eager = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# Use try/except so we can easily re-execute the whole notebook.\n",
    "if use_tf_eager:\n",
    "    try:\n",
    "        tf.enable_eager_execution()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "\n",
    "def evaluate(tensors):\n",
    "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
    "    Args:\n",
    "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
    "      `namedtuple` or combinations thereof.\n",
    "\n",
    "    Returns:\n",
    "      ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
    "        `EagerTensor`s replaced by Numpy `ndarray`s.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        return tf.contrib.framework.nest.pack_sequence_as(\n",
    "            tensors,\n",
    "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    return sess.run(tensors)\n",
    "\n",
    "\n",
    "class _TFColor(object):\n",
    "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
    "    red = '#F15854'\n",
    "    blue = '#5DA5DA'\n",
    "    orange = '#FAA43A'\n",
    "    green = '#60BD68'\n",
    "    pink = '#F17CB0'\n",
    "    brown = '#B2912F'\n",
    "    purple = '#B276B2'\n",
    "    yellow = '#DECF3F'\n",
    "    gray = '#4D4D4D'\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return [\n",
    "            self.red,\n",
    "            self.orange,\n",
    "            self.green,\n",
    "            self.blue,\n",
    "            self.pink,\n",
    "            self.brown,\n",
    "            self.purple,\n",
    "            self.yellow,\n",
    "            self.gray,\n",
    "        ][i % 9]\n",
    "\n",
    "\n",
    "TFColor = _TFColor()\n",
    "\n",
    "\n",
    "def session_options(enable_gpu_ram_resizing=False, enable_xla=False):\n",
    "    \"\"\"\n",
    "    Allowing the notebook to make use of GPUs if they're available.\n",
    "\n",
    "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear\n",
    "    algebra that optimizes TensorFlow computations.\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.log_device_placement = True\n",
    "    if enable_gpu_ram_resizing:\n",
    "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
    "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
    "        config.gpu_options.allow_growth = True\n",
    "    if enable_xla:\n",
    "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
    "        config.graph_options.optimizer_options.global_jit_level = (\n",
    "            tf.OptimizerOptions.ON_1)\n",
    "    return config\n",
    "\n",
    "\n",
    "def reset_sess(config=None):\n",
    "    \"\"\"\n",
    "    Convenience function to create the TF graph & session or reset them.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = session_options()\n",
    "    global sess\n",
    "    tf.reset_default_graph()\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "reset_sess()\n",
    "\n",
    "# from edward.models import Categorical, Mixture, Normal\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# import params\n",
    "# https://github.com/cbonnett/MDN_Edward_Keras_TF/blob/master/MDN_Edward_Keras_TF.ipynb\n",
    "# http://cbonnett.github.io/MDN.html\n",
    "# https://github.com/tensorflow/probability/blob/a4d1fbc131880ed3ffa83fe83b1d2583da90f294/tensorflow_probability/examples/jupyter_notebooks/Mixture_Density_Network_TFP.ipynb\n",
    "\n",
    "################################################################\n",
    "\n",
    "\n",
    "def plot_normal_mix(pis, mus, sigmas, ax, label='', comp=True):\n",
    "  \"\"\"Plots the mixture of Normal models to axis=ax comp=True plots all\n",
    "  components of mixture model\n",
    "  \"\"\"\n",
    "  # x = np.linspace(-10.5, 10.5, 250)\n",
    "  x = np.linspace(-0.1, 1.1, 250)\n",
    "  final = np.zeros_like(x)\n",
    "  for i, (weight_mix, mu_mix, sigma_mix) in enumerate(zip(pis, mus, sigmas)):\n",
    "    temp = stats.norm.pdf(x, mu_mix, sigma_mix) * weight_mix\n",
    "    final = final + temp\n",
    "    if comp:\n",
    "      ax.plot(x, temp, label='Normal ' + str(i))\n",
    "  ax.plot(x, final, label='Mixture of Normals ' + label)\n",
    "  ax.legend(fontsize=13)\n",
    "\n",
    "def sample_from_mixture(x, pred_weights, pred_means, pred_std, amount):\n",
    "  \"\"\"Draws samples from mixture model.\n",
    "\n",
    "  Returns 2 d array with input X and sample from prediction of mixture model.\n",
    "  \"\"\"\n",
    "  samples = np.zeros((amount, 2))\n",
    "  n_mix = len(pred_weights[0])\n",
    "  to_choose_from = np.arange(n_mix)\n",
    "  for j, (weights, means, std_devs) in enumerate(\n",
    "          zip(pred_weights, pred_means, pred_std)):\n",
    "    index = np.random.choice(to_choose_from, p=weights)\n",
    "    samples[j, 1] = np.random.normal(means[index], std_devs[index], size=1)\n",
    "    samples[j, 0] = x[j]\n",
    "    if j == amount - 1:\n",
    "      break\n",
    "  return samples\n",
    "\n",
    "\n",
    "print(20*'=~')\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "print(20*'=~')\n",
    "\n",
    "\n",
    "\n",
    "n_epoch = 10000 #1000 #20000 #20000\n",
    "# N = 4000  # number of data points  -- replaced by num_trai\n",
    "D = 5 #6  # number of features  (8 for DES, 6 for COSMOS)\n",
    "K = 3 # number of mixture components\n",
    "\n",
    "\n",
    "learning_rate = 5e-3\n",
    "\n",
    "\n",
    "num_train = 800000 #800000\n",
    "num_test = 500 #10000 #params.num_test # 32\n",
    "#\n",
    "datafile = ['DES', 'COSMOS', 'Galacticus', 'GalaxPy'][3]\n",
    "sim_obs_combine = True\n",
    "\n",
    "if sim_obs_combine: ModelName = './Model/Edward_posterior_cutsomLoss_' + datafile + '_nComp' + str(K) + \\\n",
    "                                '_ntrain' + str(num_train) + '_nepoch' + str(n_epoch) + '_lr' + \\\n",
    "                                str(learning_rate) + '_sim_obs_combine'\n",
    "else: ModelName = './Model/Edward_posterior_customLoss_' + datafile + '_nComp' + str(K) + '_ntrain' + str(\n",
    "    num_train) + '_nepoch' + str(n_epoch)  + '_lr' + str(learning_rate)  + '_obs_only'\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if datafile == 'DES' :\n",
    "  dirIn = '../data/'\n",
    "  allfiles = ['DES.train.dat', './DES5yr.nfits.dat']\n",
    "\n",
    "  Trainfiles = np.loadtxt(dirIn + allfiles[0])\n",
    "  Testfiles = np.loadtxt(dirIn + allfiles[1])\n",
    "\n",
    "  TrainshuffleOrder = np.arange(Trainfiles.shape[0])\n",
    "  np.random.shuffle(TrainshuffleOrder)\n",
    "\n",
    "  TestshuffleOrder = np.arange(Testfiles.shape[0])\n",
    "  np.random.shuffle(TestshuffleOrder)\n",
    "\n",
    "  Trainfiles = Trainfiles[TrainshuffleOrder]\n",
    "  Testfiles = Testfiles[TestshuffleOrder]\n",
    "\n",
    "  X_train = Trainfiles[:num_train, 2:10]  # color mag\n",
    "  X_test = Testfiles[:num_test, 2:10]  # color mag\n",
    "\n",
    "  xmax = np.max( [X_train.max(), X_test.max()] )\n",
    "  xmin = np.min( [X_train.min(), X_test.min()] )\n",
    "\n",
    "  X_train = (X_train - xmin)/(xmax - xmin)\n",
    "  X_test = (X_test - xmin)/(xmax - xmin)\n",
    "\n",
    "\n",
    "  y_train = Trainfiles[:num_train, 0]  # spec z\n",
    "  y_test = Testfiles[:num_test, 0]  # spec z\n",
    "\n",
    "\n",
    "  ymax = np.max( [y_train.max(), y_test.max()] )\n",
    "  ymin = np.min( [y_train.min(), y_test.min()] )\n",
    "\n",
    "  y_train = (y_train - ymin)/(ymax - ymin)\n",
    "  y_test = (y_test - ymin)/(ymax - ymin)\n",
    "\n",
    "\n",
    "\n",
    "if datafile == 'COSMOS' :\n",
    "  dirIn = '../../Data/fromJonas/'\n",
    "  allfiles = ['catalog_v0.txt', 'catalog_v1.txt', 'catalog_v2a.txt', 'catalog_v2.txt',\n",
    "              'catalog_v2b.txt', 'catalog_v3.txt'][3]\n",
    "\n",
    "\n",
    "  Trainfiles = np.loadtxt(dirIn + allfiles)\n",
    "\n",
    "  TrainshuffleOrder = np.arange(Trainfiles.shape[0])\n",
    "  np.random.shuffle(TrainshuffleOrder)\n",
    "\n",
    "  Trainfiles = Trainfiles[TrainshuffleOrder]\n",
    "\n",
    "  X_train = Trainfiles[:num_train, 2:8]  # color mag\n",
    "  X_test = Trainfiles[num_train + 1: num_train + num_test, 2:8]  # color mag\n",
    "\n",
    "  ifFlux = True\n",
    "  if ifFlux:\n",
    "      X_train = -2.5*np.log(X_train)\n",
    "      X_test = -2.5*np.log(X_test)\n",
    "\n",
    "  xmax = np.max( [X_train.max(), X_test.max()] )\n",
    "  xmin = np.min( [X_train.min(), X_test.min()] )\n",
    "\n",
    "  X_train = (X_train - xmin)/(xmax - xmin)\n",
    "  X_test = (X_test - xmin)/(xmax - xmin)\n",
    "\n",
    "  y_train = Trainfiles[:num_train, 0]  # spec z\n",
    "  y_test = Trainfiles[num_train + 1: num_train + num_test, 0]  # spec z\n",
    "\n",
    "  ymax = np.max( [y_train.max(), y_test.max()] )\n",
    "  ymin = np.min( [y_train.min(), y_test.min()] )\n",
    "\n",
    "  y_train = (y_train - ymin)/(ymax - ymin)\n",
    "  y_test = (y_test - ymin)/(ymax - ymin)\n",
    "\n",
    "\n",
    "\n",
    "if datafile == 'GalaxPy':\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import sys\n",
    "    import glob\n",
    "    from astropy.table import Table\n",
    "\n",
    "    # path_program = os.path.dirname(os.path.dirname(os.path.realpath(__file__))) + '/'\n",
    "    path_program = '../../Data/fromGalaxev/photozs/datasets/'\n",
    "\n",
    "    class Curated_sample():\n",
    "        ''' Class to store the redshift and colors of observed galaxies,\n",
    "            and the redshift, Mpeak, colors, and \"weights\" of simulated\n",
    "            galaxies whose colors are compatible with those of observed\n",
    "            galaxies.\n",
    "\n",
    "            The observed sample include galaxies from SDSS\n",
    "            (SDSS+BOSS+eBOSS), DEEP2, and VIPERS.\n",
    "\n",
    "            The simulated sample was created by sampling the parameter of\n",
    "            GALAXPY using a LH.\n",
    "\n",
    "            The weights of simulated galaxies are related to the number\n",
    "            density of observed galaxies in the same region of the color\n",
    "            space.\n",
    "\n",
    "            You only have to care about the method load_structure. '''\n",
    "\n",
    "        def __init__(self):\n",
    "            self.arr_c = []\n",
    "            self.arr_z = []\n",
    "            self.arr_m = []\n",
    "            self.arr_w = []\n",
    "\n",
    "        def append(self, c, z, m, w):\n",
    "            self.arr_c.append(c)\n",
    "            self.arr_z.append(z)\n",
    "            self.arr_m.append(m)\n",
    "            self.arr_w.append(w)\n",
    "\n",
    "        def ndarray(self):\n",
    "            self.arr_c = np.concatenate(self.arr_c)\n",
    "            self.arr_z = np.concatenate(self.arr_z)\n",
    "            self.arr_m = np.concatenate(self.arr_m)\n",
    "            self.arr_w = np.concatenate(self.arr_w)\n",
    "\n",
    "        def save_struct(self, name):\n",
    "            np.save(name + 'c.npy', self.arr_c)\n",
    "            np.save(name + 'z.npy', self.arr_z)\n",
    "            np.save(name + 'm.npy', self.arr_m)\n",
    "            np.save(name + 'w.npy', self.arr_w)\n",
    "\n",
    "        def load_struct(self, name):\n",
    "            self.arr_c = np.load(name + 'c.npy')\n",
    "            self.arr_z = np.load(name + 'z.npy')\n",
    "            self.arr_m = np.load(name + 'm.npy')\n",
    "            self.arr_w = np.load(name + 'w.npy')\n",
    "\n",
    "        def duplicate_data(self, zrange):\n",
    "            aa = np.where((self.arr_w > 50)\n",
    "                          & (self.arr_z >= zrange[0])\n",
    "                          & (self.arr_z < zrange[1]))[0]\n",
    "            print(aa.shape)\n",
    "            cc = np.repeat(aa, self.arr_w[aa].astype(int))\n",
    "            self.arr_cn = self.arr_c[cc, :]\n",
    "            self.arr_zn = self.arr_z[cc]\n",
    "            self.arr_mn = self.arr_m[cc]\n",
    "\n",
    "\n",
    "    def read_curated_data():\n",
    "        run_path = path_program + 'runs/run_z3/'\n",
    "\n",
    "        sim_q = Curated_sample()  # simulated colors quenched galaxies\n",
    "        sim_s = Curated_sample()  # simulated colors star-forming galaxies\n",
    "        obs_q = Curated_sample()  # observed colors quenched galaxies\n",
    "        obs_s = Curated_sample()  # observed colors star-forming galaxies\n",
    "\n",
    "        obs_q.load_struct(run_path + 'str_obs_q')\n",
    "        obs_s.load_struct(run_path + 'str_obs_s')\n",
    "        sim_q.load_struct(run_path + 'str_sim_q')\n",
    "        sim_s.load_struct(run_path + 'str_sim_s')\n",
    "\n",
    "        print(sim_q.arr_c.shape)\n",
    "        print(sim_s.arr_c.shape)\n",
    "        print(obs_q.arr_c.shape)\n",
    "        print(obs_s.arr_c.shape)\n",
    "\n",
    "        return sim_q, sim_s, obs_q, obs_s\n",
    "\n",
    "\n",
    "    sim_q, sim_s, obs_q, obs_s = read_curated_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if sim_obs_combine:\n",
    "        train_datafile = 'GalaxPy'\n",
    "        \n",
    "\n",
    "    # 2.0 ####### TRAIN USING SIMULATION, TEST OBSERVATION ####\n",
    "\n",
    "        Trainfiles =np.append( sim_q.arr_c, sim_s.arr_c, axis = 0)\n",
    "        TrainZ = np.append( sim_q.arr_z, sim_s.arr_z, axis = 0)\n",
    "\n",
    "        Trainfiles = np.delete(Trainfiles,(4), axis=1)   ## deleting z-Y\n",
    "\n",
    "        Testfiles =np.append( obs_q.arr_c, obs_s.arr_c, axis = 0)\n",
    "        TestZ = np.append( obs_q.arr_z, obs_s.arr_z, axis = 0)\n",
    "\n",
    "\n",
    "        TrainshuffleOrder = np.arange(Trainfiles.shape[0])\n",
    "        np.random.shuffle(TrainshuffleOrder)\n",
    "\n",
    "        Trainfiles = Trainfiles[TrainshuffleOrder]\n",
    "        TrainZ = TrainZ[TrainshuffleOrder]\n",
    "\n",
    "        TestshuffleOrder = np.arange(Testfiles.shape[0])\n",
    "        np.random.shuffle(TestshuffleOrder)\n",
    "\n",
    "        Testfiles = Testfiles[TestshuffleOrder]\n",
    "        TestZ = TestZ[TestshuffleOrder]\n",
    "\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[:num_test]  # color mag\n",
    "\n",
    "        y_train = TrainZ[:num_train]  # spec z\n",
    "        y_test = TrainZ[:num_test]  # spec z\n",
    "\n",
    "    else:\n",
    "        train_datafile = 'SDSS'\n",
    "        # 1.1 ####### SIMULATED: QUENCHED ONLY ############\n",
    "        # Trainfiles = sim_q.arr_c\n",
    "        # TrainZ = sim_q.arr_z\n",
    "\n",
    "        # 1.2 ### SIMULATED: QUENCHED + STAR FORMATION ####\n",
    "\n",
    "        # Trainfiles =np.append( sim_q.arr_c, sim_s.arr_c, axis = 0)\n",
    "        # TrainZ = np.append( sim_q.arr_z, sim_s.arr_z, axis = 0)\n",
    "\n",
    "\n",
    "        # 1.3 ####### OBSERVED: QUENCHED + STAR FORMATION ####\n",
    "\n",
    "        Trainfiles =np.append( obs_q.arr_c, obs_s.arr_c, axis = 0)\n",
    "        TrainZ = np.append( obs_q.arr_z, obs_s.arr_z, axis = 0)\n",
    "\n",
    "        TrainshuffleOrder = np.arange(Trainfiles.shape[0])\n",
    "        np.random.shuffle(TrainshuffleOrder)\n",
    "\n",
    "        Trainfiles = Trainfiles[TrainshuffleOrder]\n",
    "        TrainZ = TrainZ[TrainshuffleOrder]\n",
    "\n",
    "        # 1 #################################\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[num_train + 1: num_train + num_test]  # color mag\n",
    "\n",
    "        X_train = Trainfiles[:num_train]  # color mag\n",
    "        X_test = Trainfiles[num_train + 1: num_train + num_test]  # color mag\n",
    "\n",
    "        y_train = TrainZ[:num_train]  # spec z\n",
    "        y_test = TrainZ[num_train + 1: num_train + num_test]  # spec z\n",
    "\n",
    "\n",
    "\n",
    "    ############## THINGS ARE SAME AFTER THIS ###########\n",
    "\n",
    "    ## rescaling xmax/xmin\n",
    "    xmax = np.max([np.max(X_train, axis = 0), np.max(X_test, axis=0)], axis = 0)\n",
    "    xmin = np.min([np.min(X_train, axis = 0), np.min(X_test, axis=0)], axis = 0)\n",
    "\n",
    "    X_train = (X_train - xmin) / (xmax - xmin)\n",
    "    X_test = (X_test - xmin) / (xmax - xmin)\n",
    "\n",
    "    #### RESCALING X_train, X_test NOT done yet -- (g-i), (r-i) ... and i mag -->> Color/Mag issue\n",
    "\n",
    "    ymax = np.max([y_train.max(), y_test.max()])\n",
    "    ymin = np.min([y_train.min(), y_test.min()])\n",
    "\n",
    "    y_train = (y_train - ymin) / (ymax - ymin)\n",
    "    y_test = (y_test - ymin) / (ymax - ymin)\n",
    "\n",
    "\n",
    "print(\"Size of features in training data: {}\".format(X_train.shape))\n",
    "print(\"Size of output in training data: {}\".format(y_train.shape))\n",
    "print(\"Size of features in test data: {}\".format(X_test.shape))\n",
    "print(\"Size of output in test data: {}\".format(y_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "PlotScatter = False\n",
    "\n",
    "if PlotScatter:\n",
    "    for ind in range(D):\n",
    "\n",
    "        sns.regplot(X_train[:,ind], y_train, fit_reg=False, scatter_kws={'s':8})\n",
    "        # sns.regplot(X_train[:,1], y_train, fit_reg=False, color='blue', scatter_kws={'s':8})\n",
    "        # sns.regplot(X_train[:,2], y_train, fit_reg=False, color='green', scatter_kws={'s':8})\n",
    "        # sns.regplot(X_train[:,3], y_train, fit_reg=False, color='black', scatter_kws={'s':8})\n",
    "        # sns.regplot(X_train[:,4], y_train, fit_reg=False, color='orange', scatter_kws={'s':8})\n",
    "        # sns.regplot(X_train[:,5], y_train, fit_reg=False, color='gray', scatter_kws={'s':8})\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-3698e1d3c507>:47: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /anaconda3/envs/tf_tfp/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################### network arch ##############################\n",
    "\n",
    "#\n",
    "# X_ph = tf.placeholder(tf.float32, [None, D])\n",
    "# y_ph = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "\n",
    "# def neural_network(X):\n",
    "#   \"\"\"loc, scale, logits = NN(x; theta)\"\"\"\n",
    "#   # 2 hidden layers with 15 hidden units\n",
    "#   net = tf.layers.dense(X, 15, activation=tf.nn.relu)\n",
    "#   net = tf.layers.dense(net, 15, activation=tf.nn.relu)\n",
    "#   locs = tf.layers.dense(net, K, activation=None)\n",
    "#   scales = tf.layers.dense(net, K, activation=tf.exp)\n",
    "#   logits = tf.layers.dense(net, K, activation=None)\n",
    "#   return locs, scales, logits\n",
    "#\n",
    "#\n",
    "# locs, scales, logits = neural_network(X_ph)\n",
    "# # cat = ed.Categorical(logits=logits)\n",
    "# cat = tfd.Categorical(logits=logits)\n",
    "#\n",
    "# components = [ed.Normal(loc=loc, scale=scale) for loc, scale\n",
    "#               in zip(tf.unstack(tf.transpose(locs)),\n",
    "#                      tf.unstack(tf.transpose(scales)))]\n",
    "# y = ed.Mixture(cat=cat, components=components, value=tf.zeros_like(y_ph))\n",
    "# # Note: A bug exists in Mixture which prevents samples from it to have\n",
    "# # a shape of [None]. For now fix it using the value argument, as\n",
    "# # sampling is not necessary for MAP estimation anyways.\n",
    "\n",
    "\n",
    "def neural_network(X):\n",
    "    \"\"\"\n",
    "    loc, scale, logits = NN(x; theta)\n",
    "\n",
    "    Args:\n",
    "      X: Input Tensor containing input data for the MDN\n",
    "    Returns:\n",
    "      locs: The means of the normal distributions that our data is divided into.\n",
    "      scales: The scales of the normal distributions that our data is divided\n",
    "        into.\n",
    "      logits: The probabilities of ou categorical distribution that decides\n",
    "        which normal distribution our data points most probably belong to.\n",
    "    \"\"\"\n",
    "    # 2 hidden layers with 15 hidden units\n",
    "    net = tf.layers.dense(X, 15, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 15, activation=tf.nn.relu)\n",
    "    locs = tf.layers.dense(net, K, activation=None)\n",
    "    scales = tf.layers.dense(net, K, activation=tf.exp)\n",
    "    logits = tf.layers.dense(net, K, activation=None)\n",
    "    return locs, scales, logits\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "\n",
    "locs, scales, logits = neural_network(tf.convert_to_tensor(X_train))\n",
    "cat = tfd.Categorical(logits=logits)\n",
    "components = [tfd.Normal(loc=loc, scale=scale) for loc, scale\n",
    "              in zip(tf.unstack(tf.transpose(locs)),\n",
    "                     tf.unstack(tf.transpose(scales)))]\n",
    "\n",
    "y = tfd.Mixture(cat=cat, components=components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ######################### inference ##############################\n",
    "\n",
    "# # # There are no latent variables to infer. Thus inference is concerned\n",
    "# # # with only training model parameters, which are baked into how we\n",
    "# # # specify the neural networks.\n",
    "# # inference = ed.MAP(data={y: y_ph})\n",
    "# # optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# # inference.initialize(optimizer=optimizer, var_list=tf.trainable_variables())\n",
    "\n",
    "\n",
    "\n",
    "# # There are no latent variables to infer. Thus inference is concerned\n",
    "# # with only training model parameters, which are baked into how we\n",
    "# # specify the neural networks.\n",
    "\n",
    "# log_likelihood = y.log_prob(y_train)\n",
    "# log_likelihood = -tf.reduce_sum(log_likelihood)\n",
    "# learning_rate = 5e-2\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# train_op = optimizer.minimize(log_likelihood)\n",
    "\n",
    "# ##################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tf_tfp/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################### inference ##############################\n",
    "# ##### custom loss trial #######\n",
    "\n",
    "log_likelihood = y.log_prob(y_train)\n",
    "# log_likelihood = -tf.reduce_sum(log_likelihood/(1. + y_train)**2 )\n",
    "log_likelihood = -tf.reduce_sum(log_likelihood/(1. + y_train) )\n",
    "\n",
    "\n",
    "# learning_rate = 5e-2\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(log_likelihood)\n",
    "\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ######################### inference ##############################\n",
    "# # ##### custom loss trial #######\n",
    "\n",
    "# log_likelihood = y.log_prob(y_train)\n",
    "# # log_likelihood = -tf.reduce_sum(log_likelihood/(1. + y_train)**2 )\n",
    "# log_likelihood = -tf.reduce_sum(log_likelihood/(1. + y_train) )\n",
    "\n",
    "\n",
    "# learning_rate = 5e-2\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# train_op = optimizer.minimize(log_likelihood)\n",
    "\n",
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### custom loss trial #######\n",
    "\n",
    "# # here's the KL-Div loss, note the inputs are softmax distributions, not raw logits\n",
    "# def kl_divergence(p, q):\n",
    "#     return tf.reduce_sum(p * tf.log(p/q))\n",
    "\n",
    "# loss_2 = kl_divergence(a, y_g)\n",
    "\n",
    "# # combined loss, since the DKL loss can be negative, reverse its sign when negative\n",
    "# # basically an abs() but the demonstration is on how to use tf.cond() to check tensor values\n",
    "# loss_2 = tf.cond(loss_2 < 0, lambda: -1 * loss_2, lambda: loss_2)\n",
    "\n",
    "# # can also normalize the losses for stability but not done in this case\n",
    "# norm = 1 #tf.reduce_sum(loss_1 + loss_2)\n",
    "# loss = loss_1 / norm + dkl_loss_rate*loss_2 / norm\n",
    "\n",
    "# # optimizer used to compute gradient of loss and apply the parameter updates.\n",
    "# # the train_step object returned is ran by a TF Session to train the net\n",
    "\n",
    "# train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sess = ed.get_session()\n",
    "# tf.global_variables_initializer().run()\n",
    "\n",
    "#\n",
    "# train_loss = np.zeros(n_epoch)\n",
    "# test_loss = np.zeros(n_epoch)\n",
    "# for i in range(n_epoch):\n",
    "#   info_dict = inference.update(feed_dict={X_ph: X_train, y_ph: y_train})\n",
    "#   train_loss[i] = info_dict['loss']\n",
    "#   test_loss[i] = sess.run(inference.loss,\n",
    "#                           feed_dict={X_ph: X_test, y_ph: y_test})\n",
    "#   inference.print_progress(info_dict)\n",
    "\n",
    "\n",
    "#\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#\n",
    "#\n",
    "train_loss = np.zeros(n_epoch)\n",
    "test_loss = np.zeros(n_epoch)\n",
    "for i in range(n_epoch):\n",
    "#     print(\"epoch: \",i)\n",
    "    _, loss_value = evaluate([train_op, log_likelihood])\n",
    "    train_loss[i] = loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_epoch), -train_loss / len(X_train), label='Train Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_weights, pred_means, pred_std = evaluate([tf.nn.softmax(logits), locs, scales])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS IS NOT WORKING RN ######\n",
    "\n",
    "evaluate(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "log_likelihood_train = y.log_prob(y_train)\n",
    "log_likelihood_train = -tf.reduce_sum(log_likelihood_train)\n",
    "# learning_rate = 5e-2\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(log_likelihood_train)\n",
    "\n",
    "log_likelihood_test = y.log_prob(y_test)\n",
    "log_likelihood_test = -tf.reduce_sum(log_likelihood_test)\n",
    "train_op = optimizer.minimize(log_likelihood_test)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "\n",
    "evaluate(tf.global_variables_initializer())\n",
    "\n",
    "#\n",
    "train_loss = np.zeros(n_epoch)\n",
    "test_loss = np.zeros(n_epoch)\n",
    "for i in range(n_epoch):\n",
    "    print(\"epoch: \", i)\n",
    "    _, loss_value = evaluate([train_op, log_likelihood_train])\n",
    "    train_loss[i] = loss_value\n",
    "    \n",
    "    _, loss_value_test = evaluate([train_op, log_likelihood_test])\n",
    "    test_loss[i] = loss_value_test\n",
    "    \n",
    "    \n",
    "###################################################################\n",
    "## Plot log likelihood or loss\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(16, 3.5))\n",
    "plt.plot(np.arange(n_epoch), -test_loss / len(X_test), label='Test')\n",
    "plt.plot(np.arange(n_epoch), -train_loss / len(X_train), label='Train')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Log-likelihood', fontsize=15)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS ISNT WORKING RN ######\n",
    "\n",
    "#################################################\n",
    "\n",
    "# FailedPreconditionError\n",
    "###### .  Everything works with y_train, x_train\n",
    "#### including validation is rather difficult\n",
    "#### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pred_weights, pred_means, pred_std = sess.run()\n",
    "\n",
    "X_ph = tf.placeholder(tf.float32, [None, D])\n",
    "pred_weights, pred_means, pred_std = sess.run([tf.nn.softmax(logits), locs, scales], feed_dict={X_ph: X_test})\n",
    "\n",
    "\n",
    "\n",
    "#################### testing ####################\n",
    "\n",
    "X_ph_new = tf.placeholder(tf.float32, [None, D])\n",
    "locs_new, scales_new, logits_new = neural_network(X_ph_new)\n",
    "\n",
    "pred_weights, pred_means, pred_std = sess.run([tf.nn.softmax(logits_new), locs_new, scales_new], feed_dict={X_ph_new: X_test})\n",
    "# FailedPreconditionError\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "obj = [493, 12, 232]\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, sharex = True, figsize=(8, 7))\n",
    "\n",
    "plot_normal_mix(pred_weights[obj][0], pred_means[obj][0],\n",
    "                pred_std[obj][0], axes[0], comp=False)\n",
    "# axes[0].axvline(x=y_test[obj][0], color='black', alpha=0.5)\n",
    "axes[0].axvline(x=y_train[obj][0], color='black', alpha=0.5)\n",
    "\n",
    "axes[0].text(0.3, 4.0, 'ID: ' +str(obj[0]), horizontalalignment='center',\n",
    "             verticalalignment='center')\n",
    "\n",
    "\n",
    "plot_normal_mix(pred_weights[obj][1], pred_means[obj][1],\n",
    "                pred_std[obj][1], axes[1], comp=False)\n",
    "# axes[1].axvline(x=y_test[obj][1], color='black', alpha=0.5)\n",
    "axes[1].axvline(x=y_train[obj][1], color='black', alpha=0.5)\n",
    "\n",
    "axes[1].text(0.3, 4.0, 'ID: ' +str(obj[1]), horizontalalignment='center',\n",
    "             verticalalignment='center')\n",
    "\n",
    "plot_normal_mix(pred_weights[obj][2], pred_means[obj][2],\n",
    "                pred_std[obj][2], axes[2], comp=False)\n",
    "# axes[2].axvline(x=y_test[obj][2], color='black', alpha=0.5)\n",
    "axes[2].axvline(x=y_train[obj][2], color='black', alpha=0.5)\n",
    "\n",
    "axes[2].text(0.3, 4.0, 'ID: ' +str(obj[2]), horizontalalignment='center',\n",
    "             verticalalignment='center')\n",
    "\n",
    "plt.xlabel(r' rescaled[$z_{pred}]$', fontsize = 19)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "a = sample_from_mixture(X_test[:,1], pred_weights, pred_means,\n",
    "                        pred_std, amount=len(X_test))\n",
    "sns.jointplot(a[:, 0], a[:, 1], kind=\"hex\", color=\"#4CB391\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################################################3\n",
    "\n",
    "import SetPub\n",
    "SetPub.set_pub()\n",
    "\n",
    "\n",
    "## Overall mean --- weight * mean\n",
    "\n",
    "y_pred = np.sum(pred_means*pred_weights, axis = 1)\n",
    "y_pred_std = np.sum(pred_std*pred_weights, axis = 1)\n",
    "\n",
    "plt.figure(22, figsize=(9,8))\n",
    "\n",
    "\n",
    "# plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "# plt.errorbar( (ymax - ymin)*(ymin + y_train), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(ymin + y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "plt.errorbar( ((ymax - ymin)*ymin + y_train), ((ymax - ymin)*ymin + y_pred), yerr= ((ymax - ymin)*ymin + y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "plt.plot((ymax - ymin)*(ymin + y_train), (ymax - ymin)*(ymin + y_train), 'k')\n",
    "plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.title('weight x mean')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################################################\n",
    "##  mean --- highest weight\n",
    "\n",
    "\n",
    "\n",
    "weight_max = np.argmax(pred_weights, axis = 1)  ## argmax or max???\n",
    "weight_max = np.max(pred_weights, axis = 1)  ## argmax or max???\n",
    "from keras.utils import np_utils\n",
    "weight_max = np_utils.to_categorical(weight_max)\n",
    "\n",
    "\n",
    "y_pred = np.max(pred_weights*weight_max*pred_means, axis=1)\n",
    "y_pred_std = np.max(pred_weights*weight_max*pred_std, axis = 1)\n",
    "\n",
    "# y_pred = pred_weights[weight_max]*pred_means[weight_max]\n",
    "# y_pred_std = pred_weights[weight_max]*pred_std[weight_max]\n",
    "\n",
    "plt.figure(24, figsize=(9, 8))\n",
    "\n",
    "\n",
    "# plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "plt.errorbar( ((ymax - ymin)*ymin + y_train), ((ymax - ymin)*ymin + y_pred), yerr= ((ymax - ymin)*ymin + y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "\n",
    "plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "plt.plot((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_test), 'k')\n",
    "plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.title('highest weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################################################\n",
    "##  peak\n",
    "\n",
    "\n",
    "\n",
    "# weight_max = np.argmax(pred_weights, axis = 1)  ## argmax or max???\n",
    "component_max = np.argmax(pred_weights/np.sqrt(2*np.pi()*(pred_std**2)), axis = 1)  ## argmax or max???\n",
    "# weight_max = np.max(pred_weights, axis = 1)  ## argmax or max???\n",
    "component_max = np.max(pred_weights/np.sqrt(2*np.pi()*(pred_std**2)), axis = 1)  ## argmax or max???\n",
    "from keras.utils import np_utils\n",
    "componenet_max = np_utils.to_categorical(componenet_max)\n",
    "\n",
    "\n",
    "y_pred = np.max(pred_weights*weight_max*pred_means, axis=1)\n",
    "y_pred_std = np.max(pred_weights*weight_max*pred_std, axis = 1)\n",
    "\n",
    "# y_pred = pred_weights[weight_max]*pred_means[weight_max]\n",
    "# y_pred_std = pred_weights[weight_max]*pred_std[weight_max]\n",
    "\n",
    "plt.figure(24, figsize=(9, 8))\n",
    "\n",
    "\n",
    "# plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "plt.errorbar( ((ymax - ymin)*ymin + y_train), ((ymax - ymin)*ymin + y_pred), yerr= ((ymax - ymin)*ymin + y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "\n",
    "plt.text(0.2, 0.9, train_datafile + ' trained', horizontalalignment='center', verticalalignment='center')\n",
    "plt.plot((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_test), 'k')\n",
    "plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.title('highest weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#############################################################################3\n",
    "\n",
    "import SetPub\n",
    "SetPub.set_pub()\n",
    "\n",
    "\n",
    "## Overall mean --- weight * mean\n",
    "\n",
    "y_pred = np.sum(pred_means*pred_weights, axis = 1)\n",
    "y_pred_std = np.sum(pred_std*pred_weights, axis = 1)\n",
    "\n",
    "plt.figure(22)\n",
    "\n",
    "\n",
    "# plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "# plt.errorbar( (ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(ymin + y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "plt.errorbar( (ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(ymin + y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "\n",
    "\n",
    "plt.text(0.8, 2.0, datafile, horizontalalignment='center', verticalalignment='center')\n",
    "plt.plot((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_test), 'k')\n",
    "plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "# plt.xscale([0,1])\n",
    "# plt.yscale([0,1])\n",
    "plt.title('weight x mean')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "###########################################################################\n",
    "##  mean --- highest weight\n",
    "\n",
    "\n",
    "\n",
    "weight_max = np.argmax(pred_weights, axis = 1)  ## argmax or max???\n",
    "weight_max = np.max(pred_weights, axis = 1)  ## argmax or max???\n",
    "from keras.utils import np_utils\n",
    "weight_max = np_utils.to_categorical(weight_max)\n",
    "\n",
    "\n",
    "y_pred = np.max(pred_weights*weight_max*pred_means, axis=1)\n",
    "y_pred_std = np.max(pred_weights*weight_max*pred_std, axis = 1)\n",
    "\n",
    "# y_pred = pred_weights[weight_max]*pred_means[weight_max]\n",
    "# y_pred_std = pred_weights[weight_max]*pred_std[weight_max]\n",
    "\n",
    "plt.figure(24)\n",
    "\n",
    "\n",
    "# plt.scatter(y_test, y_pred, facecolors='k', s = 1)\n",
    "plt.errorbar((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_pred), yerr= (ymax - ymin)*(\n",
    "  ymin + y_pred_std), fmt='bo', ecolor='r', ms = 2, alpha = 0.1)\n",
    "\n",
    "\n",
    "plt.text(0.8, 2.0, datafile, horizontalalignment='center', verticalalignment='center')\n",
    "plt.plot((ymax - ymin)*(ymin + y_test), (ymax - ymin)*(ymin + y_test), 'k')\n",
    "plt.ylabel(r'$z_{pred}$', fontsize = 19)\n",
    "plt.xlabel(r'$z_{true}$', fontsize = 19)\n",
    "# plt.xscale([0,1])a\n",
    "# plt.yscale([0,1])\n",
    "plt.title('highest weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################################################################\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########## SAVE sess ###########\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "# with tf.Session() as sess:\n",
    "#     saver = tf.train.Saver()\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     saver.restore(sess, 'model-1')\n",
    "#\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# sess_save = ed.get_session()\n",
    "\n",
    "\n",
    "save_path = saver.save(sess, ModelName)\n",
    "print(\"Inference model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ModelName + '.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess_load:\n",
    "  new_saver = tf.train.import_meta_graph(ModelName + '.meta')\n",
    "#   new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "  new_saver.restore(sess_load, ModelName)\n",
    "  print('Model restored')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/tensorflow/probability/issues/325\n",
    "# # from tensorflow.contrib.saved_model import save_keras_model\n",
    "\n",
    "\n",
    "# #  To save:\n",
    "# file = h5py.File('models/{}.h5'.format(model_name), 'w')\n",
    "# weight = model.get_weights()\n",
    "# for i in range(len(weight)):\n",
    "#    file.create_dataset('weight' + str(i), data=weight[i])\n",
    "# file.close()\n",
    "\n",
    "\n",
    "\n",
    "# file = h5py.File('models/{}.h5'.format(model_name), 'r')\n",
    "# weight = []\n",
    "# for i in range(len(file.keys())):\n",
    "#    weight.append(file['weight' + str(i)][:])\n",
    "# model.set_weights(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### some progress here:: managed to restore the model\n",
    "X_ph_new = tf.placeholder(tf.float32, [None, D])\n",
    "y_ph_new = tf.placeholder(tf.float32, [None])\n",
    "locs_new, scales_new, logits_new = neural_network(tf.convert_to_tensor(X_test))\n",
    "\n",
    "cat_new = tfd.Categorical(logits=logits_new)\n",
    "components_new = [tfd.Normal(loc=loc, scale=scale) for loc, scale\n",
    "              in zip(tf.unstack(tf.transpose(locs_new)),\n",
    "                     tf.unstack(tf.transpose(scales_new)))]\n",
    "\n",
    "y_new = tfd.Mixture(cat=cat_new, components=components_new)\n",
    "\n",
    "\n",
    "\n",
    "sess_load = tf.Session() \n",
    "new_saver = tf.train.import_meta_graph(ModelName + '.meta')\n",
    "#   new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "new_saver.restore(sess_load, ModelName)\n",
    "print('Model restored')\n",
    "\n",
    "# print(sess_load.run('w1:0'))\n",
    "\n",
    "\n",
    "### works until here #######\n",
    "\n",
    "\n",
    "def neural_network(X):\n",
    "    \"\"\"\n",
    "    loc, scale, logits = NN(x; theta)\n",
    "\n",
    "    Args:\n",
    "      X: Input Tensor containing input data for the MDN\n",
    "    Returns:\n",
    "      locs: The means of the normal distributions that our data is divided into.\n",
    "      scales: The scales of the normal distributions that our data is divided\n",
    "        into.\n",
    "      logits: The probabilities of ou categorical distribution that decides\n",
    "        which normal distribution our data points most probably belong to.\n",
    "    \"\"\"\n",
    "    # 2 hidden layers with 15 hidden units\n",
    "    net = tf.layers.dense(X, 15, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 15, activation=tf.nn.relu)\n",
    "    locs = tf.layers.dense(net, K, activation=None)\n",
    "    scales = tf.layers.dense(net, K, activation=tf.exp)\n",
    "    logits = tf.layers.dense(net, K, activation=None)\n",
    "    return locs, scales, logits\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# locs, scales, logits = neural_network(tf.convert_to_tensor(X_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('good so far')\n",
    "\n",
    "\n",
    "pred_weights_new, pred_means_new, pred_std_new = sess_load.run(\n",
    "            [tf.nn.softmax(logits_new), locs_new, scales_new], feed_dict={X_ph_new: X_test})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.expand_dims(X_test[idx], axis=0)\n",
    "\n",
    "y_test_prob = []\n",
    "feed_dict = {x: x_test, n: 1}\n",
    "for i in range(n_samples):\n",
    "  y_test_prob.append(sess.run(probs, feed_dict=feed_dict))\n",
    "\n",
    "y_test_prob = np.squeeze(np.array(y_test_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ifTesting = False\n",
    "if ifTesting:\n",
    "##########  TESTING SCRIPT ################\n",
    "\n",
    "\n",
    "    X_ph_new = tf.placeholder(tf.float32, [None, D])\n",
    "    y_ph_new = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "\n",
    "    def neural_network(X):\n",
    "      \"\"\"loc, scale, logits = NN(x; theta)\"\"\"\n",
    "      # 2 hidden layers with 15 hidden units\n",
    "      net = tf.layers.dense(X, 15, activation=tf.nn.relu)\n",
    "      net = tf.layers.dense(net, 15, activation=tf.nn.relu)\n",
    "      locs = tf.layers.dense(net, K, activation=None)\n",
    "      scales = tf.layers.dense(net, K, activation=tf.exp)\n",
    "      logits = tf.layers.dense(net, K, activation=None)\n",
    "      return locs, scales, logits\n",
    "\n",
    "\n",
    "    locs_new, scales_new, logits_new = neural_network(X_ph_new)\n",
    "\n",
    "    cat_new = ed.Categorical(logits=logits_new)\n",
    "    components_new = [ed.Normal(loc=loc, scale=scale) for loc, scale\n",
    "                  in zip(tf.unstack(tf.transpose(locs_new)),\n",
    "                         tf.unstack(tf.transpose(scales_new)))]\n",
    "    y_new = ed.Mixture(cat=cat_new, components=components_new, value=tf.zeros_like(y_ph_new))\n",
    "    ## Note: A bug exists in Mixture which prevents samples from it to have\n",
    "    ## a shape of [None]. For now fix it using the value argument, as\n",
    "    ## sampling is not necessary for MAP estimation anyways.\n",
    "\n",
    "    ######################### inference ##############################\n",
    "\n",
    "    # There are no latent variables to infer. Thus inference is concerned\n",
    "    # with only training model parameters, which are baked into how we\n",
    "    # specify the neural networks.\n",
    "\n",
    "\n",
    "\n",
    "    inference_new = ed.MAP(data={y_new: y_ph_new})\n",
    "    optimizer_new = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    inference.initialize(optimizer=optimizer_new, var_list=tf.trainable_variables())\n",
    "\n",
    "\n",
    "    # new_saver = tf.train.Saver()\n",
    "    new_saver = tf.train.import_meta_graph(ModelName+'.meta')\n",
    "\n",
    "    sess_load = ed.get_session()\n",
    "    # tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "    # ModelName = './Model/Edward_posterior'\n",
    "    # new_saver = tf.train.Saver()\n",
    "\n",
    "    new_saver.restore(sess_load, ModelName)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "\n",
    "    pred_weights_new, pred_means_new, pred_std_new = sess_load.run(\n",
    "        [tf.nn.softmax(logits_new), locs_new, scales_new], feed_dict={X_ph_new: X_test})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    obj = [93, 402, 120]\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, sharex = True, figsize=(8, 7))\n",
    "\n",
    "    plot_normal_mix(pred_weights_new[obj][0], pred_means_new[obj][0],\n",
    "                    pred_std_new[obj][0], axes[0], comp=False)\n",
    "    axes[0].axvline(x=y_test[obj][0], color='black', alpha=0.5)\n",
    "    axes[0].text(0.3, 4.0, 'ID: ' +str(obj[0]), horizontalalignment='center',\n",
    "                 verticalalignment='center')\n",
    "\n",
    "\n",
    "    plot_normal_mix(pred_weights_new[obj][1], pred_means_new[obj][1],\n",
    "                    pred_std_new[obj][1], axes[1], comp=False)\n",
    "    axes[1].axvline(x=y_test[obj][1], color='black', alpha=0.5)\n",
    "    axes[1].text(0.3, 4.0, 'ID: ' +str(obj[1]), horizontalalignment='center',\n",
    "                 verticalalignment='center')\n",
    "\n",
    "    plot_normal_mix(pred_weights_new[obj][2], pred_means_new[obj][2],\n",
    "                    pred_std_new[obj][2], axes[2], comp=False)\n",
    "    axes[2].axvline(x=y_test[obj][2], color='black', alpha=0.5)\n",
    "    axes[2].text(0.3, 4.0, 'ID: ' +str(obj[2]), horizontalalignment='center',\n",
    "                 verticalalignment='center')\n",
    "\n",
    "    plt.xlabel(r' rescaled[$z_{pred}]$', fontsize = 19)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_tfp]",
   "language": "python",
   "name": "conda-env-tf_tfp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
